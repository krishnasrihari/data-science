{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging python in the world of Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import hadoopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are generating more and more data day by day. We have generated more data this century than we have in the previous century and we  are currently only 15 years into this century. Big Data is the new buzz word and everyone is talking about it. It brings new possibilities. Google translate is able to translate any language thanks to Big data. We are able to decode our Human Genome. We can predict the failure of a turbine and do the required maintainence, thanks to Big Data.\n",
    "\n",
    "There are 3 Vs of Big Data and they are defined as follows.\n",
    "\n",
    "1. Volume - This defines the size of the data. Facebook has petabytes of data about their users\n",
    "2. Velocity - This is the rate at which the data is generated. \n",
    "3. Variety - Data is not only in table form. There is data from text, images and sound. Data comes in the form of json, xml and other  types\n",
    "\n",
    "In this chapter, we'll learn how to use python in the world of Big Data by\n",
    "1. Understanding Hadoop\n",
    "2. Writing a Map Reduce program in Python\n",
    "3. Using Pydoop \n",
    "4. Understanding Spark\n",
    "5. Writing a spark program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Apache Hadoop website, Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\n",
    "\n",
    "<img src=\"files/images/hadoop_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-Reduce is a programming paradigm that takes a large distributed computation as a sequence of distributed operations on large data sets of key-value pairs. The Map-Reduce framework takes a cluster of machines and executes Map-Reduce jobs across the machines in the cluster. A Map-Reduce job has two phases, a mapping phase and a reduce phase. The input to the Map Reduce is a data set of key/value pairs.\n",
    "\n",
    "In the mapping phase,  Hadoop splits the input data set into a large number of fragments and assigns each fragment to a mapping task. Hadoop also distributes the many map tasks across the cluster of machines on which it operates. Each mapping task takes the key-value pairs from its assigned fragment and generates a set of intermediate key-value pairs. For each input key-value pair, the map task invokes a user defined mapping function that transforms the input into a different key-value pair .\n",
    "\n",
    "Following the mapping phase the hadoop sorts the intermediate data set by key and produces a set of key value tuples so that all the values associated with a particular key appear together. It also divides the set of tuples into a number of fragments equal to the number of reduce tasks.\n",
    "\n",
    "In the reduce phase, each reduce task consumes the fragment of key value tuples assigned to it. For each such tuple, it invokes a  reduce function that transforms the tuple into an output key/value pair. The hadoop framework distributes the many reduce tasks across the cluster of machines and deals with giving the appropriate fragment of intermediate data to each of the reduce task.\n",
    "\n",
    "<img src=\"files/images/MapReduce.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a master/slave architecture. It has a single master server - jobtracker and several slave servers - tasktrackers, one per machine/node in the cluster. The jobtracker is the point of communication between users and the framework. Users submit map-reduce jobs to the jobtracker, which puts them in a queue of pending jobs and executes them on a first-come/first-served basis. The jobtracker manages the assignment of map and reduce tasks to the tasktrackers. The tasktrackers execute tasks upon instruction from the jobtracker and also handle data motion between the map and reduce phases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop's Distributed File System is designed to store very large files across machines in a large cluster. It has been inspired by the Google File System. Hadoop DFS stores each file as a sequence of blocks, all blocks in a file except the last block are the same size. Blocks belonging to a file are replicated for fault tolerance. The block size and replication factor are configurable per file. Files in HDFS are \"write once\" and have strictly one writer at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop DFS Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Hadoop Map/Reduce, HDFS follows a master/slave architecture. An HDFS installation consists of a single Namenode, a master server that manages the filesystem namespace and regulates access to files by clients. In addition, there are a number of Datanodes, one per node in the cluster, which manage storage attached to the nodes that they run on. The Namenode makes filesystem namespace operations like opening, closing, renaming etc. of files and directories available via an RPC interface. It also determines the mapping of blocks to Datanodes. The Datanodes are responsible for serving read and write requests from filesystem clients, they also perform block creation, deletion, and replication upon instruction from the Namenode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Map-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation of Hadoop won't be covered in this book but you can install it through the following link\n",
    "\n",
    "http://www.cloudera.com/content/cloudera/en/documentation/cdh4/latest/CDH4-Quick-Start/cdh4qs_topic_3_2.html\n",
    "\n",
    "We'll be using the Hadoop streaming api for executing our Python Map-Reduce program in Hadoop. The Hadoop Streaming API helps in using any program having standard input and output as a map reduce program.\n",
    "\n",
    "We'll be writing two Map Reduce Program with python.\n",
    "\n",
    "1. A Basic Word Count\n",
    "2. Getting the Sentiment Score of each review\n",
    "3. Getting overall sentiment score from all the reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the word count map-reduce. Save the following code in a word_mapper.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "for l in sys.stdin:\n",
    "    \n",
    "    # Trailing and Leading white space is removed\n",
    "    l = l.strip()\n",
    "    \n",
    "    # words in the line is split\n",
    "    word_tokens = l.split()\n",
    "    \n",
    "    # Key Value pair is outputted\n",
    "    for w in word_tokens:\n",
    "        print '%s\\t%s' % (w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above mapper code, each line of the file is stripped of the leading and trailing white spaces. The line is then into tokens of words and then  these tokens of words are outputted as key value pair of <word> 1.\n",
    "\n",
    "Save the following code in word_reducer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word_token = None\n",
    "counter = 0\n",
    "word = None\n",
    "\n",
    "# STDIN Input\n",
    "for l in sys.stdin:\n",
    "    # Trailing and Leading white space is removed\n",
    "    l = l.strip()\n",
    "\n",
    "    # input from the mapper is parsed\n",
    "    word_token, counter = l.split('\\t', 1)\n",
    "\n",
    "    # count is converted to int\n",
    "    try:\n",
    "        counter = int(counter)\n",
    "    except ValueError:\n",
    "        # if count is not a number then ignore the line\n",
    "        continue\n",
    "\n",
    "    #Since hadoop sorts the mapper output by key, the following\n",
    "    # if else statement works\n",
    "    if current_word_token == word_token:\n",
    "        current_counter += counter\n",
    "    else:\n",
    "        if current_word_token:\n",
    "            print '%s\\t%s' % (current_word_token, current_counter)\n",
    "        \n",
    "        current_counter = counter\n",
    "        current_word_token = word_token\n",
    "\n",
    "# The last word is outputed\n",
    "if current_word_token == word_token:\n",
    "    print '%s\\t%s' % (current_word_token, current_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we use current_word_token to keep track of the current word that is being counted. In the for loop, we use word_token and counter to get the value out of the key value pair. We then convert the counter to int type.\n",
    "\n",
    "In the if else statement, if the word_token is same as the previous instance which is current_word_token then we keep counting else  if its new word that has come then we output the word and its count. The last if statement is to output the last word. \n",
    "\n",
    "We can check out if the mapper is working fine by the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolly\t1\n",
      "dolly\t1\n",
      "max\t1\n",
      "max\t1\n",
      "jack\t1\n",
      "tim\t1\n",
      "max\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo 'dolly dolly max max jack tim max' | ./BigData/word_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the reducer is also working fine by piping the reducer to the sorted list of the mapper output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolly\t2\n",
      "jack\t1\n",
      "max\t3\n",
      "tim\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"dolly dolly max max jack tim max\" | ./BigData/word_mapper.py | sort -k1,1  | ./BigData/word_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to apply the same on a local file containing the summary of moby dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t28\n",
      "A\t2\n",
      "abilities\t1\n",
      "aboard\t3\n",
      "about\t2\n",
      "adorned\t1\n",
      "Africa\t1\n",
      "Africa,\t1\n",
      "after\t1\n",
      "after,\t2\n",
      "After\t1\n",
      "again\t3\n",
      "again,\t1\n",
      "Ahab\t13\n",
      "Ahab,\t1\n",
      "Ahab’s\t6\n",
      "ahead.\t1\n",
      "All\t1\n",
      "alone\t1\n",
      "also\t1\n",
      "always\t1\n",
      "American\t1\n",
      "an\t4\n",
      "and\t36\n",
      "angry\t1\n",
      "announces\t2\n",
      "another\t1\n",
      "anticipation\t1\n",
      "anyone\t2\n",
      "appearance\t2\n",
      "appreciate\t1\n",
      "approaches\t1\n",
      "are\t7\n",
      "arm\t1\n",
      "as\t7\n",
      "As\t1\n",
      "at\t2\n",
      "At\t1\n",
      "atop\t1\n",
      "attacks\t3\n",
      "away\t1\n",
      "back\t1\n",
      "bad\t1\n",
      "balancing\t1\n",
      "baptizes\t1\n",
      "bargain\t1\n",
      "be\t3\n",
      "because\t1\n",
      "becomes\t2\n",
      "bed\t1\n",
      "Bedford,\t1\n",
      "before\t1\n",
      "beginning\t1\n",
      "begins\t1\n",
      "behind\t1\n",
      "berths\t1\n",
      "between\t1\n",
      "Bildad\t1\n",
      "Bildad,\t1\n",
      "black\t1\n",
      "blood\t1\n",
      "boat\t2\n",
      "boat,\t1\n",
      "boat.\t1\n",
      "boats\t3\n",
      "bones\t1\n",
      "Boomer,\t2\n",
      "both\t1\n",
      "boy,\t1\n",
      "buoy.\t1\n",
      "but\t4\n",
      "by\t6\n",
      "cabin\t1\n",
      "can\t1\n",
      "cannot\t1\n",
      "capital\t1\n",
      "captain,\t1\n",
      "Captain\t1\n",
      "captains\t1\n",
      "captains.\t1\n",
      "captured\t1\n",
      "carpenter\t1\n",
      "carries\t1\n",
      "carry\t1\n",
      "caught\t3\n",
      "chase,\t1\n",
      "Christmas\t1\n",
      "coffin\t2\n",
      "coffin,\t1\n",
      "cold\t1\n",
      "comes\t1\n",
      "companion.\t1\n",
      "confrontation\t1\n",
      "considers\t1\n",
      "constant\t1\n",
      "constitute\t1\n",
      "continues\t1\n",
      "corpse\t1\n",
      "countries\t1\n",
      "covered\t1\n",
      "crazed\t1\n",
      "crazy\t1\n",
      "created\t1\n",
      "crew\t2\n",
      "crew,\t1\n",
      "crewmen\t1\n",
      "cutting\t1\n",
      "day,\t2\n",
      "Day\t1\n",
      "death,\t1\n",
      "death.\t3\n",
      "deaths.\t1\n",
      "decide\t1\n",
      "deck,\t1\n",
      "declares\t2\n",
      "defiance\t1\n",
      "Delight,\t1\n",
      "demands\t1\n",
      "desire\t2\n",
      "destroy\t1\n",
      "destroying\t1\n",
      "Dick\t6\n",
      "Dick,\t2\n",
      "Dick.\t6\n",
      "die\t1\n",
      "different\t1\n",
      "disaster.\t1\n",
      "discuss\t1\n",
      "diving\t1\n",
      "doom\t1\n",
      "doubloon\t1\n",
      "dragged\t1\n",
      "drain\t1\n",
      "drive\t1\n",
      "drowns—a\t1\n",
      "During\t2\n",
      "earlier\t1\n",
      "electrical\t1\n",
      "embodiment\t1\n",
      "emerges\t1\n",
      "encounter\t4\n",
      "encounter,\t1\n",
      "encounters\t3\n",
      "end\t1\n",
      "Enderby,\t1\n",
      "ends,\t1\n",
      "enough\t1\n",
      "enters\t1\n",
      "equator,\t1\n",
      "escape\t1\n",
      "eventually\t2\n",
      "evil.\t1\n",
      "exotic-looking\t1\n",
      "expectation\t1\n",
      "expects\t1\n",
      "experience\t1\n",
      "falls\t3\n",
      "false\t1\n",
      "far\t1\n",
      "fatal\t1\n",
      "Fedallah\t1\n",
      "Fedallah,\t1\n",
      "Fedallah.\t1\n",
      "Fedallah’s\t2\n",
      "ferry\t1\n",
      "fervent\t1\n",
      "few\t1\n",
      "finally\t1\n",
      "find\t2\n",
      "fire.\t1\n",
      "first\t5\n",
      "floats\t1\n",
      "for\t7\n",
      "foreshadowing\t1\n",
      "forged\t1\n",
      "free\t1\n",
      "from\t12\n",
      "From\t1\n",
      "full,\t1\n",
      "Gabriel,\t1\n",
      "generosity\t1\n",
      "gingerly\t1\n",
      "goes\t1\n",
      "gold\t1\n",
      "great\t2\n",
      "grim\t1\n",
      "group\t1\n",
      "habits\t1\n",
      "had\t1\n",
      "hangings.\t1\n",
      "happy\t1\n",
      "hard\t1\n",
      "harpoon\t9\n",
      "harpooned,\t1\n",
      "harpooner\t1\n",
      "harpooners,\t1\n",
      "harpooners.\t1\n",
      "has\t5\n",
      "have\t4\n",
      "he\t8\n",
      "He\t7\n",
      "head\t1\n",
      "head,\t1\n",
      "head.\t1\n",
      "hearses\t1\n",
      "hearses,\t1\n",
      "help\t1\n",
      "hemp\t1\n",
      "her\t1\n",
      "him\t2\n",
      "his\t15\n",
      "His\t1\n",
      "hits\t1\n",
      "hold.\t1\n",
      "hopes\t1\n",
      "however,\t1\n",
      "hunt\t1\n",
      "hunt,\t2\n",
      "hunted\t1\n",
      "hunted.\t1\n",
      "hurled\t1\n",
      "ill\t1\n",
      "illuminating\t1\n",
      "imminent\t1\n",
      "in\t14\n",
      "Indian\t1\n",
      "industry.\t1\n",
      "information\t1\n",
      "inn\t1\n",
      "inn.\t1\n",
      "insane\t1\n",
      "intensify,\t1\n",
      "intent\t1\n",
      "interprets\t1\n",
      "into\t3\n",
      "is\t14\n",
      "Ishmael\t1\n",
      "Ishmael,\t2\n",
      "Issuing\t1\n",
      "it\t3\n",
      "it.\t2\n",
      "jaw.\t1\n",
      "Jeroboam,\t1\n",
      "jester\t1\n",
      "jumps\t1\n",
      "kill\t1\n",
      "killed\t1\n",
      "killing\t1\n",
      "kills\t1\n",
      "kind\t1\n",
      "lashed\t1\n",
      "last\t1\n",
      "launched,\t1\n",
      "leader\t1\n",
      "leaves\t1\n",
      "left\t1\n",
      "leg\t1\n",
      "leg,\t2\n",
      "legendary\t1\n",
      "lies\t1\n",
      "life\t1\n",
      "line\t1\n",
      "line,\t1\n",
      "line.\t1\n",
      "long\t1\n",
      "losing\t1\n",
      "lost\t2\n",
      "lowered\t1\n",
      "lust\t1\n",
      "mad\t2\n",
      "made\t4\n",
      "make\t1\n",
      "makes\t1\n",
      "man\t2\n",
      "maneuver\t1\n",
      "man’s\t1\n",
      "many\t1\n",
      "Massachusetts,\t1\n",
      "mast\t1\n",
      "masthead\t1\n",
      "mate,\t1\n",
      "mean\t1\n",
      "meets\t1\n",
      "men\t4\n",
      "men,\t1\n",
      "men’s\t1\n",
      "mention\t1\n",
      "met\t1\n",
      "middle\t1\n",
      "Moby\t14\n",
      "more\t2\n",
      "more.\t1\n",
      "must\t1\n",
      "mysterious\t1\n",
      "nails\t1\n",
      "named\t2\n",
      "Nantucket\t1\n",
      "Nantucket,\t1\n",
      "narrator,\t1\n",
      "New\t1\n",
      "next\t1\n",
      "no\t2\n",
      "none\t2\n",
      "not\t1\n",
      "Not\t1\n",
      "now\t1\n",
      "occurrence\t1\n",
      "ocean\t1\n",
      "ocean.\t1\n",
      "Ocean.\t1\n",
      "of\t25\n",
      "oil\t1\n",
      "oil.\t1\n",
      "omen\t1\n",
      "on\t8\n",
      "On\t1\n",
      "once\t3\n",
      "one\t2\n",
      "One\t1\n",
      "only\t1\n",
      "orders\t1\n",
      "other\t1\n",
      "out\t1\n",
      "overboard\t1\n",
      "owners,\t1\n",
      "Pacific\t1\n",
      "Peleg\t1\n",
      "Peleg.\t1\n",
      "Pequod\t9\n",
      "Pequod,\t2\n",
      "Pequod’s\t5\n",
      "picked\t1\n",
      "Pip\t1\n",
      "Pip,\t1\n",
      "popped\t1\n",
      "predictions\t1\n",
      "predicts\t1\n",
      "private\t1\n",
      "prize\t1\n",
      "processed\t1\n",
      "prophecy\t1\n",
      "prophet\t1\n",
      "prophetic\t2\n",
      "pulled\t1\n",
      "pursue\t1\n",
      "Quaker\t1\n",
      "Queequeg\t2\n",
      "(Queequeg\t1\n",
      "Queequeg.\t1\n",
      "Queequeg’s\t2\n",
      "quest.\t1\n",
      "races.\t1\n",
      "Rachel\t1\n",
      "Rachel,\t1\n",
      "rams\t1\n",
      "rather\t1\n",
      "recently\t1\n",
      "recovering\t1\n",
      "recovers,\t1\n",
      "remaining\t1\n",
      "replacement\t1\n",
      "repulsed\t1\n",
      "result\t1\n",
      "rips\t1\n",
      "rope.\t1\n",
      "rounds\t1\n",
      "sailor\t1\n",
      "sailors\t1\n",
      "sails\t1\n",
      "salary.\t1\n",
      "Samuel\t1\n",
      "savage-looking\t1\n",
      "saves\t1\n",
      "sea,\t1\n",
      "searching\t1\n",
      "second\t1\n",
      "secure\t1\n",
      "see\t2\n",
      "seek\t1\n",
      "seem\t1\n",
      "seen\t1\n",
      "sees\t1\n",
      "sent\t1\n",
      "several\t2\n",
      "share\t1\n",
      "ship\t8\n",
      "ship.\t1\n",
      "ship’s\t5\n",
      "ships,\t2\n",
      "shocking\t1\n",
      "sight\t1\n",
      "sighted\t2\n",
      "sights\t1\n",
      "sign\t1\n",
      "simply\t1\n",
      "Since\t1\n",
      "sink.\t1\n",
      "sinking\t2\n",
      "sinks\t1\n",
      "skills\t1\n",
      "skipper,\t1\n",
      "slowly\t1\n",
      "smuggled\t1\n",
      "some\t1\n",
      "soon\t1\n",
      "Soon\t2\n",
      "South\t1\n",
      "southern\t1\n",
      "sperm\t4\n",
      "spirit,\t1\n",
      "Starbuck\t1\n",
      "Starbuck,\t1\n",
      "stays\t1\n",
      "still\t2\n",
      "storm\t1\n",
      "strange\t1\n",
      "success,\t1\n",
      "successfully\t1\n",
      "survived\t1\n",
      "survives.\t1\n",
      "take\t1\n",
      "takes\t2\n",
      "Tashtego\t1\n",
      "Tashtego,\t1\n",
      "tattoos),\t1\n",
      "teeth\t1\n",
      "terms\t1\n",
      "that\t6\n",
      "the\t83\n",
      "The\t11\n",
      "their\t4\n",
      "them.\t1\n",
      "then\t2\n",
      "there\t1\n",
      "There\t1\n",
      "these\t1\n",
      "These\t1\n",
      "they\t1\n",
      "They\t2\n",
      "third\t1\n",
      "this\t2\n",
      "those\t1\n",
      "threatens\t1\n",
      "three\t1\n",
      "thrown\t1\n",
      "time\t1\n",
      "time,\t1\n",
      "tip\t1\n",
      "to\t24\n",
      "together.\t1\n",
      "took\t1\n",
      "toward\t1\n",
      "traditional\t1\n",
      "trapped\t1\n",
      "travels\t1\n",
      "trying\t1\n",
      "two\t4\n",
      "typhoon\t1\n",
      "under\t1\n",
      "understand\t1\n",
      "unsuccessfully\t1\n",
      "until\t1\n",
      "up\t3\n",
      "vengeance.\t1\n",
      "vessel\t1\n",
      "vessel.\t1\n",
      "vessels.\t1\n",
      "voluminous\t1\n",
      "vortex\t1\n",
      "voyage,\t1\n",
      "voyage.\t1\n",
      "voyages\t1\n",
      "warmer\t1\n",
      "was\t2\n",
      "waters,\t1\n",
      "weight,\t1\n",
      "whale\t7\n",
      "whale,\t1\n",
      "whale;\t1\n",
      "whale.\t4\n",
      "whaleboat\t1\n",
      "whaleboats\t1\n",
      "whaler.\t1\n",
      "whalers’\t1\n",
      "whales\t2\n",
      "whale’s\t2\n",
      "whales.\t2\n",
      "whaling\t6\n",
      "what\t1\n",
      "where\t3\n",
      "which\t6\n",
      "While\t1\n",
      "whirlpool,\t1\n",
      "white\t1\n",
      "who\t7\n",
      "whom\t1\n",
      "whose\t1\n",
      "will\t7\n",
      "with\t10\n",
      "wood,\t1\n",
      "words\t1\n",
      "work\t1\n",
      "wreck,\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./Data/mobydick_summary.txt | ./BigData/word_mapper.py | sort -k1,1  | ./BigData/word_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had written a program in the previous chapter to calculate the sentiment score, We'll extend that to write a map reduce program to determine the sentiment score for each review. Write the following code in senti_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "positive_words = open('positive-words.txt').read().split('\\n')\n",
    "negative_words = open('negative-words.txt').read().split('\\n')\n",
    "\n",
    "def sentiment_score(text, pos_list, neg_list):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in pos_list: positive_score+=1\n",
    "        if w in neg_list: negative_score+=1\n",
    "\n",
    "    return positive_score - negative_score\n",
    "\n",
    "\n",
    "for l in sys.stdin:\n",
    "    \n",
    "    # Trailing and Leading white space is removed\n",
    "    l = l.strip()\n",
    "\n",
    "    #Convert to lower case\n",
    "    l = l.lower()\n",
    "\n",
    "    #Getting the sentiment score\t\n",
    "    score = sentiment_score(l, positive_words, negative_words)\n",
    "    \n",
    "    # Key Value pair is outputted\n",
    "    print '%s\\t%s' % (l, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we re use the sentiment score function from the previous chapter. For each line, we strip the leading and trailing white spaces and then get the sentiment score for review. Finally, we output the sentence and the score.\n",
    "\n",
    "For this program, we don't require a reducer as we are calculating the sentiment in the mapper itself and we just have to output the sentiment score.\n",
    "\n",
    "Lets' test out the mapper is working fine locally with a file containing the reviews for Jurassic World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is plenty here to divert, but little to leave you enraptored. such is the fate of the sequel: bigger. louder. fewer teeth.\t0\n",
      "if you limit your expectations for jurassic world to \"more teeth,\" it will deliver on that promise. if you dare to hope for anything more-relatable characters, narrative coherence-you'll only set yourself up for disappointment.\t-1\n",
      "there's a problem when the most complex character in a film is the dinosaur\t-2\n",
      "not so much another bloated sequel as it is the fruition of dreams deferred in the previous films. too bad the genre dictates that those dreams are once again destined for disaster.\t-2\n",
      "a perfectly fine movie and entertaining enough to keep you watching until the closing credits.\t4\n",
      "this fourth installment of the jurassic park film series shows some wear and tear, but there is still some gas left in the tank. time is spent to set up the next film in the series. they will keep making more of these until we stop watching.\t0\n",
      "an angry movie with a tragic moral ... meta-adoration and criticism ends with a genetically modified dinosaur fighting off waves of dinosaurs.\t-3\n",
      "\"jurassic world,\" like its predecessors, fills up the screen with roaring, slathering, earth-shaking dinosaurs, then fills in mere humans around the edges. it's a formula that works as well in 2015 as it did in 1993.\t3\n",
      "pratt is shaping up as the go-to-guy when hollywood needs someone equally handy with a quip, a firearm and a vintage motorcycle... howard's [is] an even more impressive feat given that she has to sprint away from a rapacious dinosaur in high heels.\t3\n",
      "not quite a spielberg-quality blockbuster, but it'll do.\t0\n",
      "it combines first class effects, a genetically engineered deadly dinosaur, outstanding action, well defined characters and a screenplay that refreshes themes of corporate greed and playing god.\t0\n",
      "while the 3d beasts are undeniably impressive, their human counterparts remain resolutely two-dimensional thanks to a script that mistakes tone-deaf jumps and starts for emotional arcs.\t-1\n",
      "the avengers, mad max and tom cruise suddenly have some jurassic-sized competition for the most fun ride at the movies this summer...thanks dr. hammond!\t0\n",
      "it can't be the amazing revelation the first film was, but \"jurassic world\" is a great amusement park ride...and the best sequel to it's predecessor we've had.\t4\n",
      "jurassic world is a big, shiny, and entertaining roller coaster ride, though the 'world' is more interesting than the people.\t2\n",
      "jurassic world fills the 3d screen with summer-movie thrills and fun.\t1\n",
      "the nice way to put it is to say it's the second best film of the franchise. it's also accurate to say that it's a pale imitation of the original that's worse on every level.\t1\n",
      "trevorrow is having fun taking as little of this as seriously as he needs to.\t1\n",
      "it's a thrilling summer blockbuster and should be a roaring success at the box office to boot.\t3\n",
      "nothing will ever top the original, of course, but this sequel is certainly leagues better than the others.\t2\n",
      "the underlying theme of man's hubristic drive to control-and commercialize-nature's primal power never gets in the way of its full-throttle fun and its cavalcade of chills, thrills and stupendous state-of-the-art effects.\t4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./Data/jurassic_world_review.txt | ./BigData/senti_mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our program is able to calculate the sentiment score well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sentiment Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the overall sentiment score, we would require the reducer and we'll use the same mapper but with slight modifications.\n",
    "\n",
    "The following is the mapper code that we'll use stored in overall_senti_mapper.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "positive_words = open('./Data/positive-words.txt').read().split('\\n')\n",
    "negative_words = open('./Data/negative-words.txt').read().split('\\n')\n",
    "\n",
    "def sentiment_score(text, pos_list, neg_list):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in pos_list: positive_score+=1\n",
    "        if w in neg_list: negative_score+=1\n",
    "\n",
    "    return positive_score - negative_score\n",
    "\n",
    "\n",
    "for l in sys.stdin:\n",
    "    \n",
    "    # Trailing and Leading white space is removed\n",
    "    l = l.strip()\n",
    "\n",
    "    #Convert to lower case\n",
    "    l = l.lower()\n",
    "\n",
    "    #Getting the sentiment score\t\n",
    "    score = sentiment_score(l, positive_words, negative_words)\n",
    "\n",
    "    #Hashing the review to use it as a string\n",
    "    hash_object = hashlib.md5(l)\n",
    "    \n",
    "    # Key Value pair is outputted\n",
    "    print '%s\\t%s' % (hash_object.hexdigest(), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper code is similar to the previous mapper code and but here we are MD5 hashing the review and then outputting it as the key.\n",
    "\n",
    "Following is the reducer coder that utilize to determine the overall sentiment about the movie. Store the following code in overall_senti_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "# STDIN Input\n",
    "for l in sys.stdin:\n",
    "   \n",
    "    # input from the mapper is parsed\n",
    "    key, score = l.split('\\t', 1)\n",
    "\n",
    "    # count is converted to int\n",
    "    try:\n",
    "        score = int(score)\n",
    "    except ValueError:\n",
    "        # if score is not a number then ignore the line\n",
    "        continue\n",
    "\n",
    "    #Updating the total score\t\n",
    "    total_score += score\n",
    "\n",
    "\n",
    "print '%s' % (total_score,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we strip out the value containing the score and we then keep adding to the total_score variable. Finally, we output the total_score variable which shows the sentiment of the movie.\n",
    "\n",
    "Let's test out locally the overall sentiment on Jurassic World which is a good movie and then test it out on the movie Unfinished Business which was critically poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./Data/jurassic_world_review.txt | ./BigData/overall_senti_mapper.py | sort -k1,1  | ./BigData/overall_senti_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./Data/unfinished_business_review.txt | ./BigData/overall_senti_mapper.py | sort -k1,1  | ./BigData/overall_senti_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our code is working well and we also see that Jurassic World has a more positive score which means people liked it a lot and Unfinished Business has negative value which shows that people didn't like it much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Map Reduce code on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a directory Moby Dick, Jurassic World and Unfinished Business Data in HDFS tmp folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -mkdir /tmp/moby_dick\n",
    "hadoop fs -mkdir /tmp/jurassic_world\n",
    "hadoop fs -mkdir /tmp/unfinished_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the folders are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxrwxrwx   - mapred hadoop          0 2014-11-14 15:42 /tmp/hadoop-mapred\n",
      "drwxr-xr-x   - samzer hadoop          0 2015-06-18 18:31 /tmp/jurassic_world\n",
      "drwxrwxrwx   - hdfs   hadoop          0 2014-11-14 15:41 /tmp/mapred\n",
      "drwxr-xr-x   - samzer hadoop          0 2015-06-18 18:31 /tmp/moby_dick\n",
      "drwxr-xr-x   - samzer hadoop          0 2015-06-16 18:17 /tmp/temp635459726\n",
      "drwxr-xr-x   - samzer hadoop          0 2015-06-18 18:31 /tmp/unfinished_business\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the folders are created, lets copy the data file to the respective folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -copyFromLocal ./Data/mobydick_summary.txt /tmp/moby_dick\n",
    "hadoop fs -copyFromLocal ./Data/jurassic_world_review.txt /tmp/jurassic_world\n",
    "hadoop fs -copyFromLocal ./Data/unfinished_business_review.txt /tmp/unfinished_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the file is copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 samzer hadoop       5973 2015-06-18 18:34 /tmp/moby_dick/mobydick_summary.txt\n",
      "Found 1 items\n",
      "-rw-r--r--   3 samzer hadoop       3185 2015-06-18 18:34 /tmp/jurassic_world/jurassic_world_review.txt\n",
      "Found 1 items\n",
      "-rw-r--r--   3 samzer hadoop       2294 2015-06-18 18:34 /tmp/unfinished_business/unfinished_business_review.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls /tmp/moby_dick\n",
    "hadoop fs -ls /tmp/jurassic_world\n",
    "hadoop fs -ls /tmp/unfinished_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that files have been copied successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following command, we'll execute our mapper and reducers script in hadoop with the following command. In the following command, we define the mapper, reducer, input and output file locations and then use hadoop streaming to execute our scripts.\n",
    "\n",
    "Let's execute the word count program first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./BigData/word_mapper.py, ./BigData/word_reducer.py, /tmp/hadoop-samzer/hadoop-unjar3615191027855941954/] [] /tmp/streamjob7745994398502096828.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "15/06/18 18:39:12 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/06/18 18:39:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/06/18 18:39:14 INFO streaming.StreamJob: getLocalDirs(): [/data/1/mapred/local, /data/2/mapred/local, /data/3/mapred/local]\n",
      "15/06/18 18:39:14 INFO streaming.StreamJob: Running job: job_201506181207_0001\n",
      "15/06/18 18:39:14 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/06/18 18:39:14 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201506181207_0001\n",
      "15/06/18 18:39:14 INFO streaming.StreamJob: Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0001\n",
      "15/06/18 18:39:15 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/06/18 18:39:24 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/06/18 18:39:30 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/06/18 18:39:33 INFO streaming.StreamJob: Job complete: job_201506181207_0001\n",
      "15/06/18 18:39:33 INFO streaming.StreamJob: Output: /tmp/moby_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-*streaming*.jar -file ./BigData/word_mapper.py -mapper word_mapper.py -file ./BigData/word_reducer.py -reducer word_reducer.py -input /tmp/moby_dick/* -output /tmp/moby_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the word count map reduce program is working successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Queequeg\t1\n",
      "A\t2\n",
      "Africa\t1\n",
      "Africa,\t1\n",
      "After\t1\n",
      "Ahab\t13\n",
      "Ahab,\t1\n",
      "Ahab’s\t6\n",
      "All\t1\n",
      "American\t1\n",
      "As\t1\n",
      "At\t1\n",
      "Bedford,\t1\n",
      "Bildad\t1\n",
      "Bildad,\t1\n",
      "Boomer,\t2\n",
      "Captain\t1\n",
      "Christmas\t1\n",
      "Day\t1\n",
      "Delight,\t1\n",
      "Dick\t6\n",
      "Dick,\t2\n",
      "Dick.\t6\n",
      "During\t2\n",
      "Enderby,\t1\n",
      "Fedallah\t1\n",
      "Fedallah,\t1\n",
      "Fedallah.\t1\n",
      "Fedallah’s\t2\n",
      "From\t1\n",
      "Gabriel,\t1\n",
      "He\t7\n",
      "His\t1\n",
      "Indian\t1\n",
      "Ishmael\t1\n",
      "Ishmael,\t2\n",
      "Issuing\t1\n",
      "Jeroboam,\t1\n",
      "Massachusetts,\t1\n",
      "Moby\t14\n",
      "Nantucket\t1\n",
      "Nantucket,\t1\n",
      "New\t1\n",
      "Not\t1\n",
      "Ocean.\t1\n",
      "On\t1\n",
      "One\t1\n",
      "Pacific\t1\n",
      "Peleg\t1\n",
      "Peleg.\t1\n",
      "Pequod\t9\n",
      "Pequod,\t2\n",
      "Pequod’s\t5\n",
      "Pip\t1\n",
      "Pip,\t1\n",
      "Quaker\t1\n",
      "Queequeg\t2\n",
      "Queequeg.\t1\n",
      "Queequeg’s\t2\n",
      "Rachel\t1\n",
      "Rachel,\t1\n",
      "Samuel\t1\n",
      "Since\t1\n",
      "Soon\t2\n",
      "South\t1\n",
      "Starbuck\t1\n",
      "Starbuck,\t1\n",
      "Tashtego\t1\n",
      "Tashtego,\t1\n",
      "The\t11\n",
      "There\t1\n",
      "These\t1\n",
      "They\t2\n",
      "While\t1\n",
      "a\t28\n",
      "abilities\t1\n",
      "aboard\t3\n",
      "about\t2\n",
      "adorned\t1\n",
      "after\t1\n",
      "after,\t2\n",
      "again\t3\n",
      "again,\t1\n",
      "ahead.\t1\n",
      "alone\t1\n",
      "also\t1\n",
      "always\t1\n",
      "an\t4\n",
      "and\t36\n",
      "angry\t1\n",
      "announces\t2\n",
      "another\t1\n",
      "anticipation\t1\n",
      "anyone\t2\n",
      "appearance\t2\n",
      "appreciate\t1\n",
      "approaches\t1\n",
      "are\t7\n",
      "arm\t1\n",
      "as\t7\n",
      "at\t2\n",
      "atop\t1\n",
      "attacks\t3\n",
      "away\t1\n",
      "back\t1\n",
      "bad\t1\n",
      "balancing\t1\n",
      "baptizes\t1\n",
      "bargain\t1\n",
      "be\t3\n",
      "because\t1\n",
      "becomes\t2\n",
      "bed\t1\n",
      "before\t1\n",
      "beginning\t1\n",
      "begins\t1\n",
      "behind\t1\n",
      "berths\t1\n",
      "between\t1\n",
      "black\t1\n",
      "blood\t1\n",
      "boat\t2\n",
      "boat,\t1\n",
      "boat.\t1\n",
      "boats\t3\n",
      "bones\t1\n",
      "both\t1\n",
      "boy,\t1\n",
      "buoy.\t1\n",
      "but\t4\n",
      "by\t6\n",
      "cabin\t1\n",
      "can\t1\n",
      "cannot\t1\n",
      "capital\t1\n",
      "captain,\t1\n",
      "captains\t1\n",
      "captains.\t1\n",
      "captured\t1\n",
      "carpenter\t1\n",
      "carries\t1\n",
      "carry\t1\n",
      "caught\t3\n",
      "chase,\t1\n",
      "coffin\t2\n",
      "coffin,\t1\n",
      "cold\t1\n",
      "comes\t1\n",
      "companion.\t1\n",
      "confrontation\t1\n",
      "considers\t1\n",
      "constant\t1\n",
      "constitute\t1\n",
      "continues\t1\n",
      "corpse\t1\n",
      "countries\t1\n",
      "covered\t1\n",
      "crazed\t1\n",
      "crazy\t1\n",
      "created\t1\n",
      "crew\t2\n",
      "crew,\t1\n",
      "crewmen\t1\n",
      "cutting\t1\n",
      "day,\t2\n",
      "death,\t1\n",
      "death.\t3\n",
      "deaths.\t1\n",
      "decide\t1\n",
      "deck,\t1\n",
      "declares\t2\n",
      "defiance\t1\n",
      "demands\t1\n",
      "desire\t2\n",
      "destroy\t1\n",
      "destroying\t1\n",
      "die\t1\n",
      "different\t1\n",
      "disaster.\t1\n",
      "discuss\t1\n",
      "diving\t1\n",
      "doom\t1\n",
      "doubloon\t1\n",
      "dragged\t1\n",
      "drain\t1\n",
      "drive\t1\n",
      "drowns—a\t1\n",
      "earlier\t1\n",
      "electrical\t1\n",
      "embodiment\t1\n",
      "emerges\t1\n",
      "encounter\t4\n",
      "encounter,\t1\n",
      "encounters\t3\n",
      "end\t1\n",
      "ends,\t1\n",
      "enough\t1\n",
      "enters\t1\n",
      "equator,\t1\n",
      "escape\t1\n",
      "eventually\t2\n",
      "evil.\t1\n",
      "exotic-looking\t1\n",
      "expectation\t1\n",
      "expects\t1\n",
      "experience\t1\n",
      "falls\t3\n",
      "false\t1\n",
      "far\t1\n",
      "fatal\t1\n",
      "ferry\t1\n",
      "fervent\t1\n",
      "few\t1\n",
      "finally\t1\n",
      "find\t2\n",
      "fire.\t1\n",
      "first\t5\n",
      "floats\t1\n",
      "for\t7\n",
      "foreshadowing\t1\n",
      "forged\t1\n",
      "free\t1\n",
      "from\t12\n",
      "full,\t1\n",
      "generosity\t1\n",
      "gingerly\t1\n",
      "goes\t1\n",
      "gold\t1\n",
      "great\t2\n",
      "grim\t1\n",
      "group\t1\n",
      "habits\t1\n",
      "had\t1\n",
      "hangings.\t1\n",
      "happy\t1\n",
      "hard\t1\n",
      "harpoon\t9\n",
      "harpooned,\t1\n",
      "harpooner\t1\n",
      "harpooners,\t1\n",
      "harpooners.\t1\n",
      "has\t5\n",
      "have\t4\n",
      "he\t8\n",
      "head\t1\n",
      "head,\t1\n",
      "head.\t1\n",
      "hearses\t1\n",
      "hearses,\t1\n",
      "help\t1\n",
      "hemp\t1\n",
      "her\t1\n",
      "him\t2\n",
      "his\t15\n",
      "hits\t1\n",
      "hold.\t1\n",
      "hopes\t1\n",
      "however,\t1\n",
      "hunt\t1\n",
      "hunt,\t2\n",
      "hunted\t1\n",
      "hunted.\t1\n",
      "hurled\t1\n",
      "ill\t1\n",
      "illuminating\t1\n",
      "imminent\t1\n",
      "in\t14\n",
      "industry.\t1\n",
      "information\t1\n",
      "inn\t1\n",
      "inn.\t1\n",
      "insane\t1\n",
      "intensify,\t1\n",
      "intent\t1\n",
      "interprets\t1\n",
      "into\t3\n",
      "is\t14\n",
      "it\t3\n",
      "it.\t2\n",
      "jaw.\t1\n",
      "jester\t1\n",
      "jumps\t1\n",
      "kill\t1\n",
      "killed\t1\n",
      "killing\t1\n",
      "kills\t1\n",
      "kind\t1\n",
      "lashed\t1\n",
      "last\t1\n",
      "launched,\t1\n",
      "leader\t1\n",
      "leaves\t1\n",
      "left\t1\n",
      "leg\t1\n",
      "leg,\t2\n",
      "legendary\t1\n",
      "lies\t1\n",
      "life\t1\n",
      "line\t1\n",
      "line,\t1\n",
      "line.\t1\n",
      "long\t1\n",
      "losing\t1\n",
      "lost\t2\n",
      "lowered\t1\n",
      "lust\t1\n",
      "mad\t2\n",
      "made\t4\n",
      "make\t1\n",
      "makes\t1\n",
      "man\t2\n",
      "maneuver\t1\n",
      "many\t1\n",
      "man’s\t1\n",
      "mast\t1\n",
      "masthead\t1\n",
      "mate,\t1\n",
      "mean\t1\n",
      "meets\t1\n",
      "men\t4\n",
      "men,\t1\n",
      "mention\t1\n",
      "men’s\t1\n",
      "met\t1\n",
      "middle\t1\n",
      "more\t2\n",
      "more.\t1\n",
      "must\t1\n",
      "mysterious\t1\n",
      "nails\t1\n",
      "named\t2\n",
      "narrator,\t1\n",
      "next\t1\n",
      "no\t2\n",
      "none\t2\n",
      "not\t1\n",
      "now\t1\n",
      "occurrence\t1\n",
      "ocean\t1\n",
      "ocean.\t1\n",
      "of\t25\n",
      "oil\t1\n",
      "oil.\t1\n",
      "omen\t1\n",
      "on\t8\n",
      "once\t3\n",
      "one\t2\n",
      "only\t1\n",
      "orders\t1\n",
      "other\t1\n",
      "out\t1\n",
      "overboard\t1\n",
      "owners,\t1\n",
      "picked\t1\n",
      "popped\t1\n",
      "predictions\t1\n",
      "predicts\t1\n",
      "private\t1\n",
      "prize\t1\n",
      "processed\t1\n",
      "prophecy\t1\n",
      "prophet\t1\n",
      "prophetic\t2\n",
      "pulled\t1\n",
      "pursue\t1\n",
      "quest.\t1\n",
      "races.\t1\n",
      "rams\t1\n",
      "rather\t1\n",
      "recently\t1\n",
      "recovering\t1\n",
      "recovers,\t1\n",
      "remaining\t1\n",
      "replacement\t1\n",
      "repulsed\t1\n",
      "result\t1\n",
      "rips\t1\n",
      "rope.\t1\n",
      "rounds\t1\n",
      "sailor\t1\n",
      "sailors\t1\n",
      "sails\t1\n",
      "salary.\t1\n",
      "savage-looking\t1\n",
      "saves\t1\n",
      "sea,\t1\n",
      "searching\t1\n",
      "second\t1\n",
      "secure\t1\n",
      "see\t2\n",
      "seek\t1\n",
      "seem\t1\n",
      "seen\t1\n",
      "sees\t1\n",
      "sent\t1\n",
      "several\t2\n",
      "share\t1\n",
      "ship\t8\n",
      "ship.\t1\n",
      "ships,\t2\n",
      "ship’s\t5\n",
      "shocking\t1\n",
      "sight\t1\n",
      "sighted\t2\n",
      "sights\t1\n",
      "sign\t1\n",
      "simply\t1\n",
      "sink.\t1\n",
      "sinking\t2\n",
      "sinks\t1\n",
      "skills\t1\n",
      "skipper,\t1\n",
      "slowly\t1\n",
      "smuggled\t1\n",
      "some\t1\n",
      "soon\t1\n",
      "southern\t1\n",
      "sperm\t4\n",
      "spirit,\t1\n",
      "stays\t1\n",
      "still\t2\n",
      "storm\t1\n",
      "strange\t1\n",
      "success,\t1\n",
      "successfully\t1\n",
      "survived\t1\n",
      "survives.\t1\n",
      "take\t1\n",
      "takes\t2\n",
      "tattoos),\t1\n",
      "teeth\t1\n",
      "terms\t1\n",
      "that\t6\n",
      "the\t83\n",
      "their\t4\n",
      "them.\t1\n",
      "then\t2\n",
      "there\t1\n",
      "these\t1\n",
      "they\t1\n",
      "third\t1\n",
      "this\t2\n",
      "those\t1\n",
      "threatens\t1\n",
      "three\t1\n",
      "thrown\t1\n",
      "time\t1\n",
      "time,\t1\n",
      "tip\t1\n",
      "to\t24\n",
      "together.\t1\n",
      "took\t1\n",
      "toward\t1\n",
      "traditional\t1\n",
      "trapped\t1\n",
      "travels\t1\n",
      "trying\t1\n",
      "two\t4\n",
      "typhoon\t1\n",
      "under\t1\n",
      "understand\t1\n",
      "unsuccessfully\t1\n",
      "until\t1\n",
      "up\t3\n",
      "vengeance.\t1\n",
      "vessel\t1\n",
      "vessel.\t1\n",
      "vessels.\t1\n",
      "voluminous\t1\n",
      "vortex\t1\n",
      "voyage,\t1\n",
      "voyage.\t1\n",
      "voyages\t1\n",
      "warmer\t1\n",
      "was\t2\n",
      "waters,\t1\n",
      "weight,\t1\n",
      "whale\t7\n",
      "whale,\t1\n",
      "whale.\t4\n",
      "whale;\t1\n",
      "whaleboat\t1\n",
      "whaleboats\t1\n",
      "whaler.\t1\n",
      "whalers’\t1\n",
      "whales\t2\n",
      "whales.\t2\n",
      "whale’s\t2\n",
      "whaling\t6\n",
      "what\t1\n",
      "where\t3\n",
      "which\t6\n",
      "whirlpool,\t1\n",
      "white\t1\n",
      "who\t7\n",
      "whom\t1\n",
      "whose\t1\n",
      "will\t7\n",
      "with\t10\n",
      "wood,\t1\n",
      "words\t1\n",
      "work\t1\n",
      "wreck,\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "cat: `/tmp/moby_output/_logs': Is a directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -cat /tmp/moby_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program is working as intended. Now, we'll deploy the program of that calculates the sentiment score for each of the review. Do note we are adding the positive and negative dictionary files to the Hadoop streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./BigData/senti_mapper.py, ./BigData/senti_reducer.py, ./positive-words.txt, negative-words.txt, /tmp/hadoop-samzer/hadoop-unjar7337551354059438377/] [] /tmp/streamjob3150495844828357124.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "15/06/18 18:54:00 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/06/18 18:54:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/06/18 18:54:01 INFO streaming.StreamJob: getLocalDirs(): [/data/1/mapred/local, /data/2/mapred/local, /data/3/mapred/local]\n",
      "15/06/18 18:54:01 INFO streaming.StreamJob: Running job: job_201506181207_0002\n",
      "15/06/18 18:54:01 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/06/18 18:54:01 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201506181207_0002\n",
      "15/06/18 18:54:01 INFO streaming.StreamJob: Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0002\n",
      "15/06/18 18:54:02 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/06/18 18:54:10 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/06/18 18:54:14 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/06/18 18:54:17 INFO streaming.StreamJob: Job complete: job_201506181207_0002\n",
      "15/06/18 18:54:17 INFO streaming.StreamJob: Output: /tmp/jurassic_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-*streaming*.jar -file ./BigData/senti_mapper.py -mapper senti_mapper.py -file ./BigData/senti_reducer.py -reducer senti_reducer.py -input /tmp/jurassic_world/* -output /tmp/jurassic_output -file ./positive-words.txt -file negative-words.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if its score the sentiments of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"jurassic world,\" like its predecessors, fills up the screen with roaring, slathering, earth-shaking dinosaurs, then fills in mere humans around the edges. it's a formula that works as well in 2015 as it did in 1993.\t3\n",
      "\t\n",
      "a perfectly fine movie and entertaining enough to keep you watching until the closing credits.\t4\n",
      "\t\n",
      "an angry movie with a tragic moral ... meta-adoration and criticism ends with a genetically modified dinosaur fighting off waves of dinosaurs.\t-3\n",
      "\t\n",
      "if you limit your expectations for jurassic world to \"more teeth,\" it will deliver on that promise. if you dare to hope for anything more-relatable characters, narrative coherence-you'll only set yourself up for disappointment.\t-1\n",
      "\t\n",
      "it can't be the amazing revelation the first film was, but \"jurassic world\" is a great amusement park ride...and the best sequel to it's predecessor we've had.\t4\n",
      "\t\n",
      "it combines first class effects, a genetically engineered deadly dinosaur, outstanding action, well defined characters and a screenplay that refreshes themes of corporate greed and playing god.\t0\n",
      "\t\n",
      "it's a thrilling summer blockbuster and should be a roaring success at the box office to boot.\t3\n",
      "\t\n",
      "jurassic world fills the 3d screen with summer-movie thrills and fun.\t1\n",
      "\t\n",
      "jurassic world is a big, shiny, and entertaining roller coaster ride, though the 'world' is more interesting than the people.\t2\n",
      "\t\n",
      "not quite a spielberg-quality blockbuster, but it'll do.\t0\n",
      "\t\n",
      "not so much another bloated sequel as it is the fruition of dreams deferred in the previous films. too bad the genre dictates that those dreams are once again destined for disaster.\t-2\n",
      "\t\n",
      "nothing will ever top the original, of course, but this sequel is certainly leagues better than the others.\t2\n",
      "\t\n",
      "pratt is shaping up as the go-to-guy when hollywood needs someone equally handy with a quip, a firearm and a vintage motorcycle... howard's [is] an even more impressive feat given that she has to sprint away from a rapacious dinosaur in high heels.\t3\n",
      "\t\n",
      "the avengers, mad max and tom cruise suddenly have some jurassic-sized competition for the most fun ride at the movies this summer...thanks dr. hammond!\t0\n",
      "\t\n",
      "the nice way to put it is to say it's the second best film of the franchise. it's also accurate to say that it's a pale imitation of the original that's worse on every level.\t1\n",
      "\t\n",
      "the underlying theme of man's hubristic drive to control-and commercialize-nature's primal power never gets in the way of its full-throttle fun and its cavalcade of chills, thrills and stupendous state-of-the-art effects.\t4\n",
      "\t\n",
      "there is plenty here to divert, but little to leave you enraptored. such is the fate of the sequel: bigger. louder. fewer teeth.\t0\n",
      "\t\n",
      "there's a problem when the most complex character in a film is the dinosaur\t-2\n",
      "\t\n",
      "this fourth installment of the jurassic park film series shows some wear and tear, but there is still some gas left in the tank. time is spent to set up the next film in the series. they will keep making more of these until we stop watching.\t0\n",
      "\t\n",
      "trevorrow is having fun taking as little of this as seriously as he needs to.\t1\n",
      "\t\n",
      "while the 3d beasts are undeniably impressive, their human counterparts remain resolutely two-dimensional thanks to a script that mistakes tone-deaf jumps and starts for emotional arcs.\t-1\n",
      "\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "cat: `/tmp/jurassic_output/_logs': Is a directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -cat /tmp/jurassic_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is also working as intended. Now we'll try out the overall sentiment of a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./BigData/overall_senti_mapper.py, ./BigData/overall_senti_reducer.py, ./positive-words.txt, negative-words.txt, /tmp/hadoop-samzer/hadoop-unjar4062599583189912126/] [] /tmp/streamjob5575281473245925063.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "15/06/18 19:04:50 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/06/18 19:04:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/06/18 19:04:52 INFO streaming.StreamJob: getLocalDirs(): [/data/1/mapred/local, /data/2/mapred/local, /data/3/mapred/local]\n",
      "15/06/18 19:04:52 INFO streaming.StreamJob: Running job: job_201506181207_0005\n",
      "15/06/18 19:04:52 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/06/18 19:04:52 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201506181207_0005\n",
      "15/06/18 19:04:52 INFO streaming.StreamJob: Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0005\n",
      "15/06/18 19:04:53 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/06/18 19:05:00 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/06/18 19:05:05 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/06/18 19:05:08 INFO streaming.StreamJob: Job complete: job_201506181207_0005\n",
      "15/06/18 19:05:08 INFO streaming.StreamJob: Output: /tmp/unfinished_business_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-*streaming*.jar -file ./BigData/overall_senti_mapper.py -mapper overall_senti_mapper.py -file ./BigData/overall_senti_reducer.py -reducer overall_senti_reducer.py -input /tmp/unfinished_business/* -output /tmp/unfinished_business_output -file ./positive-words.txt -file negative-words.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "cat: `/tmp/unfinished_business_output/_logs': Is a directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -cat /tmp/unfinished_business_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overall sentiment score is coming out correctly from Map Reduce. \n",
    "\n",
    "Below is a screenshot of the Jobtracker status page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/jobtracker.png\">\n",
    "\n",
    "The image shows a portal where the jobs submitted to the jobtracker can be viewed and the status can be seen. This can be seen on port 50070 of the master system. \n",
    "\n",
    "From the image, we can see that there is a job running and the status above the image shows that the job completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Handling with Hadoopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoopy is a library in python which provides API to interact with Hadoop to manage the files and perform map reduce on it. Hadoopy can be downloaded from the following location\n",
    "\n",
    "http://www.hadoopy.com/en/latest/tutorial.html#installing-hadoopy\n",
    "\n",
    "Let's try to put few files in hadoop through hadoopy in a directory created within hdfs called data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code put the data into hdfs with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./BigData/dummy_data/test9 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test7 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test1 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test8 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test6 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test5 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test3 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test4 has been put into hdfs\n",
      "The file ./BigData/dummy_data/test2 has been put into hdfs\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import hadoopy\n",
    "import os\n",
    "\n",
    "hdfs_path = ''\n",
    "\n",
    "\n",
    "def read_local_dir(local_path):\n",
    "    for fn in os.listdir(local_path):\n",
    "        path = os.path.join(local_path, fn)\n",
    "        if os.path.isfile(path):\n",
    "            yield path\n",
    "\n",
    "\n",
    "def main():\n",
    "    local_path = './BigData/dummy_data'\n",
    "    for file in  read_local_dir(local_path):\n",
    "        hadoopy.put(file, 'data')\n",
    "        print \"The file %s has been put into hdfs\" % (file,)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we list all the files in a directory and then we put each of the file into hadoop using the put method of hadoopy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if all the files have been put into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test1\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test2\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test3\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test4\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test5\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test6\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test7\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test8\n",
      "-rw-r--r--   3 samzer hadoop          0 2015-06-23 00:19 data/test9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have successfully been able to put files into hdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/pig-logo.gif\">\n",
    "\n",
    "Pig is a platform that gives a very expressive language to perform data transformations and querying. The that is written in Pig is in a scripting manner and this gets compiled to Map Reduce Programs which executes on Hadoop. \n",
    "\n",
    "Pig helps in reducing the complexity of the raw level Map-Reduce program and enable the user to perform transformations fast.\n",
    "\n",
    "Pig latin can be learned from the this link http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html\n",
    "\n",
    "We'll be covering how to perform Top 10 most occuring words with Pig and then we show how you can create your function in python that can be used in Pig.\n",
    "\n",
    "Let's start with the Word Count. Following is the Pig Latin Code which you can save it in pig_wordcount.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load '/tmp/moby_dick/';\n",
    "\n",
    "word_token = foreach data generate flatten(TOKENIZE((chararray)$0)) as word;\n",
    "\n",
    "group_word_token = group word_token by word;\n",
    "\n",
    "count_word_token = foreach group_word_token generate COUNT(word_token) as cnt, group;\n",
    "\n",
    "sort_word_token = ORDER count_word_token by cnt DESC;\n",
    "\n",
    "top10_word_count = LIMIT sort_word_token 10; \n",
    "\n",
    "DUMP top10_word_count;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we load the summary of Moby Dick which is then Tokenized line by line which is basically splitting it into individual elements. The Flatten function converts the Collection of individual word tokens in a line to a row by row form. We then group by the words and then take a count of the words for each word. Finally we sort the counts in descending order and then we limit to the first 10 rows to get the Top 10 most occuring words.\n",
    "\n",
    "Let's execute the above pig script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83,the)\n",
      "(36,and)\n",
      "(28,a)\n",
      "(25,of)\n",
      "(24,to)\n",
      "(15,his)\n",
      "(14,Ahab)\n",
      "(14,Moby)\n",
      "(14,is)\n",
      "(14,in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015-06-20 14:31:43,395 [main] INFO  org.apache.pig.Main - Apache Pig version 0.11.0-cdh4.7.0 (rexported) compiled May 28 2014, 11:06:21\n",
      "2015-06-20 14:31:43,396 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/samzer/Coding/p/pythonDataScienceBook/pig_1434790903392.log\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "2015-06-20 14:31:43,963 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/samzer/.pigbootup not found\n",
      "2015-06-20 14:31:44,145 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-20 14:31:44,145 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:8020\n",
      "2015-06-20 14:31:44,721 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:8021\n",
      "2015-06-20 14:31:44,724 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-20 14:31:45,212 [main] WARN  org.apache.hadoop.conf.Configuration - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "2015-06-20 14:31:45,212 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.permissions.supergroup is deprecated. Instead, use dfs.permissions.superusergroup\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.max.objects is deprecated. Instead, use dfs.namenode.max.objects\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.interval is deprecated. Instead, use dfs.namenode.replication.interval\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.data.dir is deprecated. Instead, use dfs.datanode.data.dir\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.access.time.precision is deprecated. Instead, use dfs.namenode.accesstime.precision\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.min is deprecated. Instead, use dfs.namenode.replication.min\n",
      "2015-06-20 14:31:45,213 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.dir is deprecated. Instead, use dfs.namenode.checkpoint.dir\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.http.address is deprecated. Instead, use dfs.namenode.http-address\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.considerLoad is deprecated. Instead, use dfs.namenode.replication.considerLoad\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.write.packet.size is deprecated. Instead, use dfs.client-write-packet-size\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.permissions is deprecated. Instead, use dfs.permissions.enabled\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.block.size is deprecated. Instead, use dfs.blocksize\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.address is deprecated. Instead, use dfs.namenode.https-address\n",
      "2015-06-20 14:31:45,214 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.dir.restore is deprecated. Instead, use dfs.namenode.name.dir.restore\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.need.client.auth is deprecated. Instead, use dfs.client.https.need-auth\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.umaskmode is deprecated. Instead, use fs.permissions.umask-mode\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - topology.node.switch.mapping.impl is deprecated. Instead, use net.topology.node.switch.mapping.impl\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.backup.http.address is deprecated. Instead, use dfs.namenode.backup.http-address\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.secondary.http.address is deprecated. Instead, use dfs.namenode.secondary.http-address\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.safemode.extension is deprecated. Instead, use dfs.namenode.safemode.extension\n",
      "2015-06-20 14:31:45,215 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.df.interval is deprecated. Instead, use fs.df.interval\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.edits.dir is deprecated. Instead, use dfs.namenode.checkpoint.edits.dir\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.client.keystore.resource is deprecated. Instead, use dfs.client.https.keystore.resource\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.datanode.max.xcievers is deprecated. Instead, use dfs.datanode.max.transfer.threads\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.backup.address is deprecated. Instead, use dfs.namenode.backup.address\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - topology.script.number.args is deprecated. Instead, use net.topology.script.number.args\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.balance.bandwidthPerSec is deprecated. Instead, use dfs.datanode.balance.bandwidthPerSec\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.edits.dir is deprecated. Instead, use dfs.namenode.edits.dir\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.safemode.threshold.pct is deprecated. Instead, use dfs.namenode.safemode.threshold-pct\n",
      "2015-06-20 14:31:45,216 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.dir is deprecated. Instead, use dfs.namenode.name.dir\n",
      "2015-06-20 14:31:45,217 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.period is deprecated. Instead, use dfs.namenode.checkpoint.period\n",
      "2015-06-20 14:31:45,217 [main] WARN  org.apache.hadoop.conf.Configuration - hadoop.native.lib is deprecated. Instead, use io.native.lib.available\n",
      "2015-06-20 14:31:45,636 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,ORDER_BY,LIMIT\n",
      "2015-06-20 14:31:45,864 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2015-06-20 14:31:45,912 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing to move algebraic foreach to combiner\n",
      "2015-06-20 14:31:45,956 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 4\n",
      "2015-06-20 14:31:45,956 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 4\n",
      "2015-06-20 14:31:46,054 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2015-06-20 14:31:46,149 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2015-06-20 14:31:46,152 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2015-06-20 14:31:46,169 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=5973\n",
      "2015-06-20 14:31:46,169 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2015-06-20 14:31:46,170 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job4130068060732352406.jar\n",
      "2015-06-20 14:31:50,125 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job4130068060732352406.jar created\n",
      "2015-06-20 14:31:50,144 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2015-06-20 14:31:50,155 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2015-06-20 14:31:50,155 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2015-06-20 14:31:50,157 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2015-06-20 14:31:50,235 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2015-06-20 14:31:50,413 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "2015-06-20 14:31:50,738 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2015-06-20 14:31:50,868 [JobControl] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-20 14:31:50,869 [JobControl] WARN  org.apache.hadoop.conf.Configuration - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "2015-06-20 14:31:50,912 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-20 14:31:50,912 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2015-06-20 14:31:50,954 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2015-06-20 14:31:52,834 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201506181207_0007\n",
      "2015-06-20 14:31:52,834 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases count_word_token,data,group_word_token,word_token\n",
      "2015-06-20 14:31:52,834 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[1,7],word_token[3,13],count_word_token[7,19],group_word_token[5,19] C: count_word_token[7,19],group_word_token[5,19] R: count_word_token[7,19]\n",
      "2015-06-20 14:31:52,835 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0007\n",
      "2015-06-20 14:32:02,458 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 12% complete\n",
      "2015-06-20 14:32:07,999 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 25% complete\n",
      "2015-06-20 14:32:12,613 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2015-06-20 14:32:12,634 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2015-06-20 14:32:12,635 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2015-06-20 14:32:12,648 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=186087\n",
      "2015-06-20 14:32:12,649 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2015-06-20 14:32:12,650 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job6012080897166679154.jar\n",
      "2015-06-20 14:32:16,323 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job6012080897166679154.jar created\n",
      "2015-06-20 14:32:16,333 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2015-06-20 14:32:16,334 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2015-06-20 14:32:16,334 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2015-06-20 14:32:16,334 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2015-06-20 14:32:16,363 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2015-06-20 14:32:16,375 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "2015-06-20 14:32:16,789 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-20 14:32:16,790 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2015-06-20 14:32:16,793 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2015-06-20 14:32:17,830 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201506181207_0008\n",
      "2015-06-20 14:32:17,830 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases sort_word_token\n",
      "2015-06-20 14:32:17,830 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: sort_word_token[9,18] C:  R: \n",
      "2015-06-20 14:32:17,830 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0008\n",
      "2015-06-20 14:32:24,869 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 37% complete\n",
      "2015-06-20 14:32:30,402 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2015-06-20 14:32:37,453 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2015-06-20 14:32:37,475 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2015-06-20 14:32:37,476 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2015-06-20 14:32:37,476 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job8111802663263297056.jar\n",
      "2015-06-20 14:32:41,223 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job8111802663263297056.jar created\n",
      "2015-06-20 14:32:41,236 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2015-06-20 14:32:41,237 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2015-06-20 14:32:41,237 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2015-06-20 14:32:41,237 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2015-06-20 14:32:41,269 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2015-06-20 14:32:41,278 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "2015-06-20 14:32:41,754 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-20 14:32:41,754 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2015-06-20 14:32:41,756 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2015-06-20 14:32:42,731 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201506181207_0009\n",
      "2015-06-20 14:32:42,732 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases sort_word_token\n",
      "2015-06-20 14:32:42,732 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: sort_word_token[9,18] C:  R: \n",
      "2015-06-20 14:32:42,732 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0009\n",
      "2015-06-20 14:32:50,268 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 62% complete\n",
      "2015-06-20 14:32:55,797 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 75% complete\n",
      "2015-06-20 14:33:02,360 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2015-06-20 14:33:02,381 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2015-06-20 14:33:02,381 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2015-06-20 14:33:02,382 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job1979252123707690971.jar\n",
      "2015-06-20 14:33:05,974 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job1979252123707690971.jar created\n",
      "2015-06-20 14:33:05,982 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2015-06-20 14:33:05,982 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2015-06-20 14:33:05,982 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2015-06-20 14:33:05,983 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2015-06-20 14:33:05,995 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2015-06-20 14:33:06,010 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "2015-06-20 14:33:06,425 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-20 14:33:06,425 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2015-06-20 14:33:06,427 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2015-06-20 14:33:07,333 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201506181207_0010\n",
      "2015-06-20 14:33:07,334 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases sort_word_token\n",
      "2015-06-20 14:33:07,334 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: sort_word_token[9,18] C:  R: \n",
      "2015-06-20 14:33:07,334 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201506181207_0010\n",
      "2015-06-20 14:33:13,870 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 87% complete\n",
      "2015-06-20 14:33:21,932 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2015-06-20 14:33:21,937 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.0.0-cdh4.7.0\t0.11.0-cdh4.7.0\tsamzer\t2015-06-20 14:31:46\t2015-06-20 14:33:21\tGROUP_BY,ORDER_BY,LIMIT\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTIme\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_201506181207_0007\t1\t1\t4\t4\t4\t4\t5\t5\t5\t5\tcount_word_token,data,group_word_token,word_token\tGROUP_BY,COMBINER\t\n",
      "job_201506181207_0008\t1\t1\t3\t3\t3\t3\t5\t5\t5\t5\tsort_word_token\tSAMPLER\t\n",
      "job_201506181207_0009\t1\t1\t3\t3\t3\t3\t5\t5\t5\t5\tsort_word_token\tORDER_BY,COMBINER\t\n",
      "job_201506181207_0010\t1\t1\t3\t3\t3\t3\t4\t4\t4\t4\tsort_word_token\t\thdfs://localhost:8020/tmp/temp-988816745/tmp-695788691,\n",
      "\n",
      "Input(s):\n",
      "Successfully read 13 records (6348 bytes) from: \"/tmp/moby_dick\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored 10 records (116 bytes) in: \"hdfs://localhost:8020/tmp/temp-988816745/tmp-695788691\"\n",
      "\n",
      "Counters:\n",
      "Total records written : 10\n",
      "Total bytes written : 116\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_201506181207_0007\t->\tjob_201506181207_0008,\n",
      "job_201506181207_0008\t->\tjob_201506181207_0009,\n",
      "job_201506181207_0009\t->\tjob_201506181207_0010,\n",
      "job_201506181207_0010\n",
      "\n",
      "\n",
      "2015-06-20 14:33:21,997 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
      "2015-06-20 14:33:22,001 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2015-06-20 14:33:22,009 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-20 14:33:22,009 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pig ./BigData/pig_wordcount.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to get our top 10 words. Let's now create a user defined function with Python which will be used in Pig.\n",
    "\n",
    "We'll define two user defined function to score the positive and negative sentiment of a sentence. \n",
    "\n",
    "The following is the udf for score the positive sentiment and its available in positive_sentiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_words = ['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'acco$\n",
    "\n",
    "\n",
    "\n",
    "@outputSchema(\"pnum:int\")\n",
    "def sentiment_score(text):\n",
    "    positive_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in positive_words: positive_score+=1\n",
    "\n",
    "    return positive_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we define the positive word list which is used by the sentiment_score function. The function checks for the positive words in a sentence and finally outputs the total count of it. There is a outputSchema decorator which is used to tell Pig what type of data is being outputted out which in our case is int. \n",
    "\n",
    "Following is the code for scoring the negative sentiment and its available in negative_sentiment.py. The code almost similar to the positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_words = ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'ab$\n",
    "\n",
    "\n",
    "@outputSchema(\"nnum:int\")\n",
    "def sentiment_score(text):\n",
    "    negative_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in negative_words: negative_score-=1\n",
    "\n",
    "    return  negative_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the Pig which scores the sentiments of the Jurassic World review and its available in pig_sentiment.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "register 'positive_sentiment.py' using org.apache.pig.scripting.jython.JythonScriptEngine as positive;\n",
    "register 'negative_sentiment.py' using org.apache.pig.scripting.jython.JythonScriptEngine as negative;\n",
    "\n",
    "data = load '/tmp/jurassic_world/*';\n",
    "\n",
    "feedback_sentiments = foreach data generate LOWER((chararray)$0) as feedback, positive.sentiment_score(LOWER((chararray)$0)) as psenti , \n",
    "negative.sentiment_score(LOWER((chararray)$0)) as nsenti;\n",
    "\n",
    "average_sentiments = foreach feedback,feedback_sentiments generate psenti + nsenti;\n",
    "\n",
    "dump average_sentiments;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above Pig Script, we first register the python udf scripts using the register command and give it an appropriate name. We then load our Jurassic World review. We then convert our reviews to lower case and then score the positive and negative sentiments of a review. Finally, we add the score to get the overall sentiment of a review.\n",
    "\n",
    "Let's execute the Pig script and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(there is plenty here to divert, but little to leave you enraptored. such is the fate of the sequel: bigger. louder. fewer teeth.,0)\n",
      "(if you limit your expectations for jurassic world to \"more teeth,\" it will deliver on that promise. if you dare to hope for anything more-relatable characters, narrative coherence-you'll only set yourself up for disappointment.,-1)\n",
      "(there's a problem when the most complex character in a film is the dinosaur,-2)\n",
      "(not so much another bloated sequel as it is the fruition of dreams deferred in the previous films. too bad the genre dictates that those dreams are once again destined for disaster.,-2)\n",
      "(a perfectly fine movie and entertaining enough to keep you watching until the closing credits.,4)\n",
      "(this fourth installment of the jurassic park film series shows some wear and tear, but there is still some gas left in the tank. time is spent to set up the next film in the series. they will keep making more of these until we stop watching.,0)\n",
      "(an angry movie with a tragic moral ... meta-adoration and criticism ends with a genetically modified dinosaur fighting off waves of dinosaurs.,-3)\n",
      "(\"jurassic world,\" like its predecessors, fills up the screen with roaring, slathering, earth-shaking dinosaurs, then fills in mere humans around the edges. it's a formula that works as well in 2015 as it did in 1993.,3)\n",
      "(pratt is shaping up as the go-to-guy when hollywood needs someone equally handy with a quip, a firearm and a vintage motorcycle... howard's [is] an even more impressive feat given that she has to sprint away from a rapacious dinosaur in high heels.,3)\n",
      "(not quite a spielberg-quality blockbuster, but it'll do.,0)\n",
      "(it combines first class effects, a genetically engineered deadly dinosaur, outstanding action, well defined characters and a screenplay that refreshes themes of corporate greed and playing god.,0)\n",
      "(while the 3d beasts are undeniably impressive, their human counterparts remain resolutely two-dimensional thanks to a script that mistakes tone-deaf jumps and starts for emotional arcs.,-1)\n",
      "(the avengers, mad max and tom cruise suddenly have some jurassic-sized competition for the most fun ride at the movies this summer...thanks dr. hammond!,0)\n",
      "(it can't be the amazing revelation the first film was, but \"jurassic world\" is a great amusement park ride...and the best sequel to it's predecessor we've had.,4)\n",
      "(jurassic world is a big, shiny, and entertaining roller coaster ride, though the 'world' is more interesting than the people.,2)\n",
      "(jurassic world fills the 3d screen with summer-movie thrills and fun.,1)\n",
      "(the nice way to put it is to say it's the second best film of the franchise. it's also accurate to say that it's a pale imitation of the original that's worse on every level.,1)\n",
      "(trevorrow is having fun taking as little of this as seriously as he needs to.,1)\n",
      "(it's a thrilling summer blockbuster and should be a roaring success at the box office to boot.,3)\n",
      "(nothing will ever top the original, of course, but this sequel is certainly leagues better than the others.,2)\n",
      "(the underlying theme of man's hubristic drive to control-and commercialize-nature's primal power never gets in the way of its full-throttle fun and its cavalcade of chills, thrills and stupendous state-of-the-art effects.,4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015-06-22 00:10:49,026 [main] INFO  org.apache.pig.Main - Apache Pig version 0.11.0-cdh4.7.0 (rexported) compiled May 28 2014, 11:06:21\n",
      "2015-06-22 00:10:49,027 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/samzer/Coding/p/pythonDataScienceBook/pig_1434912049024.log\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "2015-06-22 00:11:28,969 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/samzer/.pigbootup not found\n",
      "2015-06-22 00:11:29,154 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-22 00:11:29,154 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:8020\n",
      "2015-06-22 00:11:29,723 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:8021\n",
      "2015-06-22 00:11:29,726 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.df.interval is deprecated. Instead, use fs.df.interval\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.max.objects is deprecated. Instead, use dfs.namenode.max.objects\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - hadoop.native.lib is deprecated. Instead, use io.native.lib.available\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.data.dir is deprecated. Instead, use dfs.datanode.data.dir\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.dir is deprecated. Instead, use dfs.namenode.name.dir\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.dir is deprecated. Instead, use dfs.namenode.checkpoint.dir\n",
      "2015-06-22 00:11:29,783 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.block.size is deprecated. Instead, use dfs.blocksize\n",
      "2015-06-22 00:11:29,784 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.access.time.precision is deprecated. Instead, use dfs.namenode.accesstime.precision\n",
      "2015-06-22 00:11:29,784 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.min is deprecated. Instead, use dfs.namenode.replication.min\n",
      "2015-06-22 00:11:29,784 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.edits.dir is deprecated. Instead, use dfs.namenode.edits.dir\n",
      "2015-06-22 00:11:29,784 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.considerLoad is deprecated. Instead, use dfs.namenode.replication.considerLoad\n",
      "2015-06-22 00:11:29,784 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.balance.bandwidthPerSec is deprecated. Instead, use dfs.datanode.balance.bandwidthPerSec\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.safemode.threshold.pct is deprecated. Instead, use dfs.namenode.safemode.threshold-pct\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.http.address is deprecated. Instead, use dfs.namenode.http-address\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.name.dir.restore is deprecated. Instead, use dfs.namenode.name.dir.restore\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.client.keystore.resource is deprecated. Instead, use dfs.client.https.keystore.resource\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.backup.address is deprecated. Instead, use dfs.namenode.backup.address\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.backup.http.address is deprecated. Instead, use dfs.namenode.backup.http-address\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.permissions is deprecated. Instead, use dfs.permissions.enabled\n",
      "2015-06-22 00:11:29,785 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.safemode.extension is deprecated. Instead, use dfs.namenode.safemode.extension\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.datanode.max.xcievers is deprecated. Instead, use dfs.datanode.max.transfer.threads\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.need.client.auth is deprecated. Instead, use dfs.client.https.need-auth\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.https.address is deprecated. Instead, use dfs.namenode.https-address\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.replication.interval is deprecated. Instead, use dfs.namenode.replication.interval\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.edits.dir is deprecated. Instead, use dfs.namenode.checkpoint.edits.dir\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.write.packet.size is deprecated. Instead, use dfs.client-write-packet-size\n",
      "2015-06-22 00:11:29,786 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.permissions.supergroup is deprecated. Instead, use dfs.permissions.superusergroup\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - topology.script.number.args is deprecated. Instead, use net.topology.script.number.args\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.umaskmode is deprecated. Instead, use fs.permissions.umask-mode\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - dfs.secondary.http.address is deprecated. Instead, use dfs.namenode.secondary.http-address\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - fs.checkpoint.period is deprecated. Instead, use dfs.namenode.checkpoint.period\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - topology.node.switch.mapping.impl is deprecated. Instead, use net.topology.node.switch.mapping.impl\n",
      "2015-06-22 00:11:29,787 [main] WARN  org.apache.hadoop.conf.Configuration - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "2015-06-22 00:11:29,799 [main] INFO  org.apache.pig.scripting.jython.JythonScriptEngine - created tmp python.cachedir=/tmp/pig_jython_6758064495736200782\n",
      "2015-06-22 00:11:32,101 [main] INFO  org.apache.pig.scripting.jython.JythonScriptEngine - Register scripting UDF: positive.sentiment_score\n",
      "2015-06-22 00:11:32,123 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-22 00:11:32,125 [main] WARN  org.apache.hadoop.conf.Configuration - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "2015-06-22 00:11:33,547 [main] INFO  org.apache.pig.scripting.jython.JythonScriptEngine - Register scripting UDF: negative.sentiment_score\n",
      "2015-06-22 00:11:34,063 [main] INFO  org.apache.pig.scripting.jython.JythonFunction - Schema 'pnum:int' defined for func sentiment_score\n",
      "2015-06-22 00:11:36,408 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN\n",
      "2015-06-22 00:11:37,581 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2015-06-22 00:11:37,606 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2015-06-22 00:11:37,606 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2015-06-22 00:11:37,729 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2015-06-22 00:11:37,814 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2015-06-22 00:11:37,817 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2015-06-22 00:11:37,825 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=3185\n",
      "2015-06-22 00:11:37,825 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2015-06-22 00:11:37,826 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job8754547545682459601.jar\n",
      "2015-06-22 00:11:45,195 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job8754547545682459601.jar created\n",
      "2015-06-22 00:11:45,213 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2015-06-22 00:11:45,220 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2015-06-22 00:11:45,220 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2015-06-22 00:11:45,222 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2015-06-22 00:11:45,264 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2015-06-22 00:11:45,703 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "2015-06-22 00:11:45,767 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2015-06-22 00:11:46,220 [JobControl] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2015-06-22 00:11:46,221 [JobControl] WARN  org.apache.hadoop.conf.Configuration - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "2015-06-22 00:11:46,267 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-22 00:11:46,267 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2015-06-22 00:11:46,293 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2015-06-22 00:11:47,478 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201506211802_0002\n",
      "2015-06-22 00:11:47,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases average_sentiments,data,feedback_sentiments\n",
      "2015-06-22 00:11:47,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[4,7],feedback_sentiments[6,22],average_sentiments[9,21] C:  R: \n",
      "2015-06-22 00:11:47,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201506211802_0002\n",
      "2015-06-22 00:13:18,181 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2015-06-22 00:14:02,516 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2015-06-22 00:14:02,518 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.0.0-cdh4.7.0\t0.11.0-cdh4.7.0\tsamzer\t2015-06-22 00:11:37\t2015-06-22 00:14:02\tUNKNOWN\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTIme\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_201506211802_0002\t1\t0\t48\t48\t48\t48\t0\t0\t0\t0\taverage_sentiments,data,feedback_sentiments\tMAP_ONLY\thdfs://localhost:8020/tmp/temp-1981532704/tmp-775527111,\n",
      "\n",
      "Input(s):\n",
      "Successfully read 21 records (3570 bytes) from: \"/tmp/jurassic_world/*\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored 21 records (3345 bytes) in: \"hdfs://localhost:8020/tmp/temp-1981532704/tmp-775527111\"\n",
      "\n",
      "Counters:\n",
      "Total records written : 21\n",
      "Total bytes written : 3345\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_201506211802_0002\n",
      "\n",
      "\n",
      "2015-06-22 00:14:02,530 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
      "2015-06-22 00:14:02,533 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2015-06-22 00:14:02,544 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2015-06-22 00:14:02,544 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pig ./BigData/pig_sentiment.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully scored the sentiments of Jurassic World Review using Python UDF in Pig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/spark.png\">\n",
    "\n",
    "\n",
    "Apache Spark is a computing framework which works on top of HDFS and provides alternative way of computing similar to Map-Reduce. It was developed by AmpLab of UC Berkeley. Spark does its computation mostly in the memory because of which it is much more faster than Map-Reduce and is well suited for Machine Learning as its able to handle Iterative Work Loads really well.\n",
    "\n",
    "\n",
    "Spark used the programming abstraction of RDDs (Resilient Distributed Datasets) in which data is logically distributed into partitions and transformations can be performed on top of it.\n",
    "\n",
    "Python is one of the language that is used for interacting with Apache Spark and we'll create a program to perform the sentiment scoring for each review of Jurassic Park as well as the overall sentiment.\n",
    "\n",
    "You can install by following the instructions in the following link.\n",
    "\n",
    "https://spark.apache.org/docs/1.0.1/spark-standalone.html\n",
    "\n",
    "The following is the Python code for scoring the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "positive_words = open('positive-words.txt').read().split('\\n')\n",
    "negative_words = open('negative-words.txt').read().split('\\n')\n",
    "\n",
    "\n",
    "def sentiment_score(text, pos_list, neg_list):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in pos_list: positive_score+=1\n",
    "        if w in neg_list: negative_score+=1\n",
    "\n",
    "    return positive_score - negative_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: sentiment <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"PythonSentiment\")\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    scores = lines.map(lambda x: (x, sentiment_score(x.lower(), positive_words, negative_words)))\n",
    "    output = scores.collect()\n",
    "    for (key, score) in output:\n",
    "        print(\"%s: %i\" % (key, score))\n",
    "\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we define our standard sentiment_score function which we'll be reusing. The if statement checks that the python script and the text file is given. The sc variable is a Spark Context object with the App name \"PythonSentiment\". The filename in the argument is passed into spark through the textFile method of sc. In the map function of Spark, we define a lambda function where each line of the text file is passed and then we obtain the line and its respective sentiment score. The output variable gets the result and finally we print the result in the screen.\n",
    "\n",
    "Let's score the sentiment of each of the review of Jurassic World. Please add your hostname below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is plenty here to divert, but little to leave you enraptored. Such is the fate of the sequel: Bigger. Louder. Fewer teeth.: 0\n",
      "If you limit your expectations for Jurassic World to \"more teeth,\" it will deliver on that promise. If you dare to hope for anything more-relatable characters, narrative coherence-you'll only set yourself up for disappointment.: -1\n",
      "There's a problem when the most complex character in a film is the dinosaur: -2\n",
      "not so much another bloated sequel as it is the fruition of dreams deferred in the previous films. Too bad the genre dictates that those dreams are once again destined for disaster.: -2\n",
      "A perfectly fine movie and entertaining enough to keep you watching until the closing credits.: 4\n",
      "This fourth installment of the Jurassic Park film series shows some wear and tear, but there is still some gas left in the tank. Time is spent to set up the next film in the series. They will keep making more of these until we stop watching.: 0\n",
      "An angry movie with a tragic moral ... meta-adoration and criticism ends with a genetically modified dinosaur fighting off waves of dinosaurs.: -3\n",
      "\"Jurassic World,\" like its predecessors, fills up the screen with roaring, slathering, earth-shaking dinosaurs, then fills in mere humans around the edges. It's a formula that works as well in 2015 as it did in 1993.: 3\n",
      "Pratt is shaping up as the go-to-guy when Hollywood needs someone equally handy with a quip, a firearm and a vintage motorcycle... Howard's [is] an even more impressive feat given that she has to sprint away from a rapacious dinosaur in high heels.: 3\n",
      "Not quite a Spielberg-quality blockbuster, but it'll do.: 0\n",
      "It combines first class effects, a genetically engineered deadly dinosaur, outstanding action, well defined characters and a screenplay that refreshes themes of corporate greed and playing God.: 0\n",
      "While the 3D beasts are undeniably impressive, their human counterparts remain resolutely two-dimensional thanks to a script that mistakes tone-deaf jumps and starts for emotional arcs.: -1\n",
      "The Avengers, Mad Max and Tom Cruise suddenly have some jurassic-sized competition for the most fun ride at the movies this summer...thanks Dr. Hammond!: 1\n",
      "It can't be the amazing revelation the first film was, but \"Jurassic World\" is a great amusement park ride...and the best sequel to it's predecessor we've had.: 4\n",
      "Jurassic World is a big, shiny, and entertaining roller coaster ride, though the 'World' is more interesting than the people.: 2\n",
      "Jurassic World fills the 3D screen with summer-movie thrills and fun.: 1\n",
      "The nice way to put it is to say it's the second best film of the franchise. It's also accurate to say that it's a pale imitation of the original that's worse on every level.: 1\n",
      "Trevorrow is having fun taking as little of this as seriously as he needs to.: 1\n",
      "It's a thrilling summer blockbuster and should be a roaring success at the box office to boot.: 3\n",
      "Nothing will ever top the original, of course, but this sequel is certainly leagues better than the others.: 2\n",
      "The underlying theme of man's hubristic drive to control-and commercialize-nature's primal power never gets in the way of its full-throttle fun and its cavalcade of chills, thrills and stupendous state-of-the-art effects.: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spark assembly has been built with Hive, including Datanucleus jars on classpath\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/06/22 18:23:55 INFO SparkContext: Running Spark version 1.3.0\n",
      "15/06/22 18:23:55 WARN Utils: Your hostname, samzer resolves to a loopback address: 127.0.0.1; using 192.168.75.156 instead (on interface wlan0)\n",
      "15/06/22 18:23:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "15/06/22 18:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/06/22 18:23:55 INFO SecurityManager: Changing view acls to: samzer\n",
      "15/06/22 18:23:55 INFO SecurityManager: Changing modify acls to: samzer\n",
      "15/06/22 18:23:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(samzer); users with modify permissions: Set(samzer)\n",
      "15/06/22 18:23:56 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/06/22 18:23:56 INFO Remoting: Starting remoting\n",
      "15/06/22 18:23:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.75.156:46991]\n",
      "15/06/22 18:23:56 INFO Utils: Successfully started service 'sparkDriver' on port 46991.\n",
      "15/06/22 18:23:56 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/06/22 18:23:56 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/06/22 18:23:56 INFO DiskBlockManager: Created local directory at /tmp/spark-556fe7ea-7940-4681-be11-d67e8cc51a65/blockmgr-43958930-983a-4a18-910f-a08e512bdcd6\n",
      "15/06/22 18:23:56 INFO MemoryStore: MemoryStore started with capacity 265.1 MB\n",
      "15/06/22 18:23:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-144b88d2-8a24-4efd-90ac-6a66bb2000c2/httpd-72297da4-3ac0-45cf-866d-27dea87767c4\n",
      "15/06/22 18:23:56 INFO HttpServer: Starting HTTP Server\n",
      "15/06/22 18:23:56 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/22 18:23:56 INFO AbstractConnector: Started SocketConnector@0.0.0.0:41613\n",
      "15/06/22 18:23:56 INFO Utils: Successfully started service 'HTTP file server' on port 41613.\n",
      "15/06/22 18:23:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/06/22 18:23:56 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/22 18:23:56 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "15/06/22 18:23:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "15/06/22 18:23:56 INFO SparkUI: Started SparkUI at http://192.168.75.156:4040\n",
      "15/06/22 18:23:57 INFO Utils: Copying /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py to /tmp/spark-91b4faa1-8969-4481-bf77-f3683a8303c2/userFiles-6c9223db-6724-4c94-82d8-a1eab765d93c/spark_sentiment.py\n",
      "15/06/22 18:23:57 INFO SparkContext: Added file file:/home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py at http://192.168.75.156:41613/files/spark_sentiment.py with timestamp 1434977637059\n",
      "15/06/22 18:23:57 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@samzer:7077/user/Master...\n",
      "15/06/22 18:23:57 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150622182357-0007\n",
      "15/06/22 18:23:57 INFO AppClient$ClientActor: Executor added: app-20150622182357-0007/0 on worker-20150622173718-192.168.75.156-44351 (192.168.75.156:44351) with 4 cores\n",
      "15/06/22 18:23:57 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150622182357-0007/0 on hostPort 192.168.75.156:44351 with 4 cores, 512.0 MB RAM\n",
      "15/06/22 18:23:57 INFO AppClient$ClientActor: Executor updated: app-20150622182357-0007/0 is now LOADING\n",
      "15/06/22 18:23:57 INFO AppClient$ClientActor: Executor updated: app-20150622182357-0007/0 is now RUNNING\n",
      "15/06/22 18:23:57 INFO NettyBlockTransferService: Server created on 47060\n",
      "15/06/22 18:23:57 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/06/22 18:23:57 INFO BlockManagerMasterActor: Registering block manager 192.168.75.156:47060 with 265.1 MB RAM, BlockManagerId(<driver>, 192.168.75.156, 47060)\n",
      "15/06/22 18:23:57 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/06/22 18:23:58 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "15/06/22 18:23:58 INFO MemoryStore: ensureFreeSpace(96581) called with curMem=0, maxMem=278019440\n",
      "15/06/22 18:23:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 94.3 KB, free 265.0 MB)\n",
      "15/06/22 18:23:58 INFO MemoryStore: ensureFreeSpace(16238) called with curMem=96581, maxMem=278019440\n",
      "15/06/22 18:23:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.9 KB, free 265.0 MB)\n",
      "15/06/22 18:23:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.75.156:47060 (size: 15.9 KB, free: 265.1 MB)\n",
      "15/06/22 18:23:58 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "15/06/22 18:23:58 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "15/06/22 18:23:59 INFO FileInputFormat: Total input paths to process : 1\n",
      "15/06/22 18:23:59 INFO SparkContext: Starting job: collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Got job 0 (collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29) with 1 output partitions (allowLocal=false)\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Final stage: Stage 0(collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29)\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Missing parents: List()\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[2] at collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29), which has no missing parents\n",
      "15/06/22 18:23:59 INFO MemoryStore: ensureFreeSpace(109200) called with curMem=112819, maxMem=278019440\n",
      "15/06/22 18:23:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 106.6 KB, free 264.9 MB)\n",
      "15/06/22 18:23:59 INFO MemoryStore: ensureFreeSpace(82964) called with curMem=222019, maxMem=278019440\n",
      "15/06/22 18:23:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 81.0 KB, free 264.8 MB)\n",
      "15/06/22 18:23:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.75.156:47060 (size: 81.0 KB, free: 265.0 MB)\n",
      "15/06/22 18:23:59 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "15/06/22 18:23:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839\n",
      "15/06/22 18:23:59 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[2] at collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29)\n",
      "15/06/22 18:23:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "15/06/22 18:24:01 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@192.168.75.156:50071/user/Executor#-1352076808] with ID 0\n",
      "15/06/22 18:24:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.75.156, PROCESS_LOCAL, 1393 bytes)\n",
      "15/06/22 18:24:02 INFO BlockManagerMasterActor: Registering block manager 192.168.75.156:33251 with 265.1 MB RAM, BlockManagerId(0, 192.168.75.156, 33251)\n",
      "15/06/22 18:24:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.75.156:33251 (size: 81.0 KB, free: 265.1 MB)\n",
      "15/06/22 18:24:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.75.156:33251 (size: 15.9 KB, free: 265.0 MB)\n",
      "15/06/22 18:24:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 16629 ms on 192.168.75.156 (1/1)\n",
      "15/06/22 18:24:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "15/06/22 18:24:18 INFO DAGScheduler: Stage 0 (collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29) finished in 18.941 s\n",
      "15/06/22 18:24:18 INFO DAGScheduler: Job 0 finished: collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_sentiment.py:29, took 19.148137 s\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}\n",
      "15/06/22 18:24:19 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}\n",
      "15/06/22 18:24:19 INFO SparkUI: Stopped Spark web UI at http://192.168.75.156:4040\n",
      "15/06/22 18:24:19 INFO DAGScheduler: Stopping DAGScheduler\n",
      "15/06/22 18:24:19 INFO SparkDeploySchedulerBackend: Shutting down all executors\n",
      "15/06/22 18:24:19 INFO SparkDeploySchedulerBackend: Asking each executor to shut down\n",
      "15/06/22 18:24:19 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!\n",
      "15/06/22 18:24:19 INFO MemoryStore: MemoryStore cleared\n",
      "15/06/22 18:24:19 INFO BlockManager: BlockManager stopped\n",
      "15/06/22 18:24:19 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "15/06/22 18:24:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorActor: OutputCommitCoordinator stopped!\n",
      "15/06/22 18:24:19 INFO SparkContext: Successfully stopped SparkContext\n",
      "15/06/22 18:24:19 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "15/06/22 18:24:19 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "~/spark-1.3.0-bin-cdh4/bin/spark-submit --master spark://<hostname>:7077 ./BigData/spark_sentiment.py hdfs://localhost:8020/tmp/jurassic_world/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our Spark Program was able to score the sentiment for each of the review. We use the Spark Submit command and we define the Spark master with the python script that needs to be executed along with the location of the Jurassic World review in hdfs.\n",
    "\n",
    "Below is a Spark program to score the overall sentiment of all the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "positive_words = open('positive-words.txt').read().split('\\n')\n",
    "negative_words = open('negative-words.txt').read().split('\\n')\n",
    "\n",
    "\n",
    "def sentiment_score(text, pos_list, neg_list):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for w in text.split(' '):\n",
    "        if w in pos_list: positive_score+=1\n",
    "        if w in neg_list: negative_score+=1\n",
    "\n",
    "    return positive_score - negative_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: Overall Sentiment <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"PythonOverallSentiment\")\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    scores = lines.map(lambda x: (\"Total\", sentiment_score(x.lower(), positive_words, negative_words)))\\\n",
    "                  .reduceByKey(add)\n",
    "    output = scores.collect()\n",
    "    for (key, score) in output:\n",
    "        print(\"%s: %i\" % (key, score))\n",
    "\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have added a reduceByKey method which reduces the value by adding them and also we have defined the Key as \"Total\" so that all the scores reduced based on the single key.\n",
    "\n",
    "Let's try out the above code to get the overall sentiment of Jurassic World. Please add your hostname below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spark assembly has been built with Hive, including Datanucleus jars on classpath\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/06/22 18:40:47 INFO SparkContext: Running Spark version 1.3.0\n",
      "15/06/22 18:40:48 WARN Utils: Your hostname, samzer resolves to a loopback address: 127.0.0.1; using 192.168.75.156 instead (on interface wlan0)\n",
      "15/06/22 18:40:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "15/06/22 18:40:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/06/22 18:40:49 INFO SecurityManager: Changing view acls to: samzer\n",
      "15/06/22 18:40:49 INFO SecurityManager: Changing modify acls to: samzer\n",
      "15/06/22 18:40:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(samzer); users with modify permissions: Set(samzer)\n",
      "15/06/22 18:40:50 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/06/22 18:40:50 INFO Remoting: Starting remoting\n",
      "15/06/22 18:40:50 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.75.156:37200]\n",
      "15/06/22 18:40:50 INFO Utils: Successfully started service 'sparkDriver' on port 37200.\n",
      "15/06/22 18:40:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/06/22 18:40:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/06/22 18:40:50 INFO DiskBlockManager: Created local directory at /tmp/spark-0a8f7472-ab04-4e1e-97b5-2795256ea815/blockmgr-a784b0ec-9e0a-4e8b-b26c-a3cdef558018\n",
      "15/06/22 18:40:50 INFO MemoryStore: MemoryStore started with capacity 265.1 MB\n",
      "15/06/22 18:40:51 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9af10a02-9c55-46a0-8030-ab877ce53471/httpd-b5ecef3b-11f4-4e42-9441-a38151b8c47a\n",
      "15/06/22 18:40:51 INFO HttpServer: Starting HTTP Server\n",
      "15/06/22 18:40:51 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/22 18:40:51 INFO AbstractConnector: Started SocketConnector@0.0.0.0:45550\n",
      "15/06/22 18:40:51 INFO Utils: Successfully started service 'HTTP file server' on port 45550.\n",
      "15/06/22 18:40:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/06/22 18:40:51 INFO Server: jetty-8.y.z-SNAPSHOT\n",
      "15/06/22 18:40:51 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "15/06/22 18:40:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "15/06/22 18:40:51 INFO SparkUI: Started SparkUI at http://192.168.75.156:4040\n",
      "15/06/22 18:40:51 INFO Utils: Copying /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py to /tmp/spark-7dd1acc7-495b-4ba7-8999-c8c81780d27b/userFiles-a07bb5c8-e186-481f-a299-c5bacf0bd504/spark_overall_sentiment.py\n",
      "15/06/22 18:40:51 INFO SparkContext: Added file file:/home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py at http://192.168.75.156:45550/files/spark_overall_sentiment.py with timestamp 1434978651666\n",
      "15/06/22 18:40:52 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@samzer:7077/user/Master...\n",
      "15/06/22 18:40:52 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150622184052-0008\n",
      "15/06/22 18:40:52 INFO AppClient$ClientActor: Executor added: app-20150622184052-0008/0 on worker-20150622173718-192.168.75.156-44351 (192.168.75.156:44351) with 4 cores\n",
      "15/06/22 18:40:52 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150622184052-0008/0 on hostPort 192.168.75.156:44351 with 4 cores, 512.0 MB RAM\n",
      "15/06/22 18:40:52 INFO AppClient$ClientActor: Executor updated: app-20150622184052-0008/0 is now LOADING\n",
      "15/06/22 18:40:52 INFO AppClient$ClientActor: Executor updated: app-20150622184052-0008/0 is now RUNNING\n",
      "15/06/22 18:40:53 INFO NettyBlockTransferService: Server created on 48200\n",
      "15/06/22 18:40:53 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/06/22 18:40:53 INFO BlockManagerMasterActor: Registering block manager 192.168.75.156:48200 with 265.1 MB RAM, BlockManagerId(<driver>, 192.168.75.156, 48200)\n",
      "15/06/22 18:40:53 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/06/22 18:40:53 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "15/06/22 18:40:54 INFO MemoryStore: ensureFreeSpace(96581) called with curMem=0, maxMem=278019440\n",
      "15/06/22 18:40:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 94.3 KB, free 265.0 MB)\n",
      "15/06/22 18:40:54 INFO MemoryStore: ensureFreeSpace(16238) called with curMem=96581, maxMem=278019440\n",
      "15/06/22 18:40:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.9 KB, free 265.0 MB)\n",
      "15/06/22 18:40:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.75.156:48200 (size: 15.9 KB, free: 265.1 MB)\n",
      "15/06/22 18:40:54 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "15/06/22 18:40:54 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "15/06/22 18:40:55 INFO FileInputFormat: Total input paths to process : 1\n",
      "15/06/22 18:40:55 INFO SparkContext: Starting job: collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:29)\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Got job 0 (collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30) with 1 output partitions (allowLocal=false)\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Final stage: Stage 1(collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30)\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Parents of final stage: List(Stage 0)\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Missing parents: List(Stage 0)\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Submitting Stage 0 (PairwiseRDD[4] at reduceByKey at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:29), which has no missing parents\n",
      "15/06/22 18:40:55 INFO MemoryStore: ensureFreeSpace(112696) called with curMem=112819, maxMem=278019440\n",
      "15/06/22 18:40:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 110.1 KB, free 264.9 MB)\n",
      "15/06/22 18:40:55 INFO MemoryStore: ensureFreeSpace(85579) called with curMem=225515, maxMem=278019440\n",
      "15/06/22 18:40:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 83.6 KB, free 264.8 MB)\n",
      "15/06/22 18:40:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.75.156:48200 (size: 83.6 KB, free: 265.0 MB)\n",
      "15/06/22 18:40:55 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "15/06/22 18:40:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839\n",
      "15/06/22 18:40:55 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PairwiseRDD[4] at reduceByKey at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:29)\n",
      "15/06/22 18:40:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "15/06/22 18:40:57 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@192.168.75.156:45050/user/Executor#-233537208] with ID 0\n",
      "15/06/22 18:40:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.75.156, PROCESS_LOCAL, 1390 bytes)\n",
      "15/06/22 18:40:57 INFO BlockManagerMasterActor: Registering block manager 192.168.75.156:60099 with 265.1 MB RAM, BlockManagerId(0, 192.168.75.156, 60099)\n",
      "15/06/22 18:40:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.75.156:60099 (size: 83.6 KB, free: 265.1 MB)\n",
      "15/06/22 18:41:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.75.156:60099 (size: 15.9 KB, free: 265.0 MB)\n",
      "15/06/22 18:41:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13048 ms on 192.168.75.156 (1/1)\n",
      "15/06/22 18:41:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "15/06/22 18:41:10 INFO DAGScheduler: Stage 0 (reduceByKey at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:29) finished in 14.433 s\n",
      "15/06/22 18:41:10 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/06/22 18:41:10 INFO DAGScheduler: running: Set()\n",
      "15/06/22 18:41:10 INFO DAGScheduler: waiting: Set(Stage 1)\n",
      "15/06/22 18:41:10 INFO DAGScheduler: failed: Set()\n",
      "15/06/22 18:41:10 INFO DAGScheduler: Missing parents for Stage 1: List()\n",
      "15/06/22 18:41:10 INFO DAGScheduler: Submitting Stage 1 (PythonRDD[7] at collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30), which is now runnable\n",
      "15/06/22 18:41:10 INFO MemoryStore: ensureFreeSpace(4744) called with curMem=311094, maxMem=278019440\n",
      "15/06/22 18:41:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 264.8 MB)\n",
      "15/06/22 18:41:10 INFO MemoryStore: ensureFreeSpace(3406) called with curMem=315838, maxMem=278019440\n",
      "15/06/22 18:41:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 264.8 MB)\n",
      "15/06/22 18:41:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.75.156:48200 (size: 3.3 KB, free: 265.0 MB)\n",
      "15/06/22 18:41:10 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "15/06/22 18:41:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:839\n",
      "15/06/22 18:41:10 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (PythonRDD[7] at collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30)\n",
      "15/06/22 18:41:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "15/06/22 18:41:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.75.156, PROCESS_LOCAL, 1126 bytes)\n",
      "15/06/22 18:41:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.75.156:60099 (size: 3.3 KB, free: 265.0 MB)\n",
      "15/06/22 18:41:10 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 0 to sparkExecutor@192.168.75.156:45050\n",
      "15/06/22 18:41:10 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 143 bytes\n",
      "15/06/22 18:41:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 248 ms on 192.168.75.156 (1/1)\n",
      "15/06/22 18:41:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "15/06/22 18:41:10 INFO DAGScheduler: Stage 1 (collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30) finished in 0.249 s\n",
      "15/06/22 18:41:10 INFO DAGScheduler: Job 0 finished: collect at /home/samzer/Coding/p/pythonDataScienceBook/./BigData/spark_overall_sentiment.py:30, took 14.927193 s\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}\n",
      "15/06/22 18:41:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}\n",
      "15/06/22 18:41:10 INFO SparkUI: Stopped Spark web UI at http://192.168.75.156:4040\n",
      "15/06/22 18:41:10 INFO DAGScheduler: Stopping DAGScheduler\n",
      "15/06/22 18:41:10 INFO SparkDeploySchedulerBackend: Shutting down all executors\n",
      "15/06/22 18:41:11 INFO SparkDeploySchedulerBackend: Asking each executor to shut down\n",
      "15/06/22 18:41:11 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!\n",
      "15/06/22 18:41:11 INFO MemoryStore: MemoryStore cleared\n",
      "15/06/22 18:41:11 INFO BlockManager: BlockManager stopped\n",
      "15/06/22 18:41:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "15/06/22 18:41:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorActor: OutputCommitCoordinator stopped!\n",
      "15/06/22 18:41:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "15/06/22 18:41:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "15/06/22 18:41:11 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "15/06/22 18:41:11 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "~/spark-1.3.0-bin-cdh4/bin/spark-submit --master spark://<hostname>:7077 ./BigData/spark_overall_sentiment.py hdfs://localhost:8020/tmp/jurassic_world/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Spark gave an overall sentiment score of 19.\n",
    "\n",
    "The applications getting executed on Spark can viewed in the browser on the 8080 port of the Spark master. Following is a screenshot of it.\n",
    "\n",
    "<img src=\"files/images/spark_monitor.png\">\n",
    "\n",
    "\n",
    "We can see that the number of nodes of Spark, applications that are getting executed currently and as well as the applications that completed execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
